{"./":{"url":"./","title":"Remark","keywords":"","body":"Remark Note Python/Scikit-Learn/Datasets/Generations.md 中，make-multilabel-classification 涉及到多标签分类（样本可能属于多个标签），make-biclusters 和 make-checkerboard 涉及到双聚类算法。待后续涉及到相关模块，再补齐这 3 个 generateions。 Python/Scikit-Learn/Datasets/Generations.md 中，回归模块的数据构造待 model-selection 模块后再补充。 Question 为什么多个无序类别特征（包括多用户）要使用笛卡尔积组合，而不是直接拼接在一起。 例如，两个特征分别包含3，2个类别，那么组合的向量为 3*2=6维，而不是3+2=5维； "},"Content/Mathematic/":{"url":"Content/Mathematic/","title":"1. Mathematic","keywords":"","body":"1. Mathematic "},"Content/Mathematic/Math-Analysis/":{"url":"Content/Mathematic/Math-Analysis/","title":"1.1. Math Analysis","keywords":"","body":"1.1 Math Analysis "},"Content/Mathematic/Linear-Algebra/":{"url":"Content/Mathematic/Linear-Algebra/","title":"1.2. Linear Algebra","keywords":"","body":"1.2 Linear Algebra "},"Content/Statistics/":{"url":"Content/Statistics/","title":"2. Statistics","keywords":"","body":"2.1 Statistics "},"Content/Time-Series-Analysis/":{"url":"Content/Time-Series-Analysis/","title":"3. Time Series Analysis","keywords":"","body":"2.2 Time Series Analysis "},"Content/Machine-Learning/":{"url":"Content/Machine-Learning/","title":"4. Machine Learning","keywords":"","body":"2.3 Machine Learning "},"Content/Machine-Learning/Technology/":{"url":"Content/Machine-Learning/Technology/","title":"4.1. Technology","keywords":"","body":"2.3.1. Technology "},"Content/Machine-Learning/Technology/机器学习三要素.html":{"url":"Content/Machine-Learning/Technology/机器学习三要素.html","title":"I - 机器学习三要素","keywords":"","body":" 机器学习三要素 1. 模型集合 2. 目标策略 2.1. 损失函数(loss function) 2.2. 经验风险(empirical risk) 2.3. 结构风险(structural risk) 3. 学习算法 机器学习三要素 个人比较认同《统计学习方法》中，李航 博士给出的机器学习的定义： 机器学习 = 模型集合 + 目标策略 + 学习算法 模型集合、目标策略、学习算法 就是机器学习三要素。 1. 模型集合 这里的 “模型集合”，是指为解决某种问题而构建的模型集合，重点强调 “集合”。例如，我们需要判别出垃圾邮件，这是一个分类问题。那么，首先我们把所有分类模型形成一个集合，这是我们后续 “目标策略” 和 “学习算法” 的基础。 2. 目标策略 确定了 “模型集合” 以后，接下来我们需要确定评价这些模型的标准，即我们怎么评判这些 “模型” 的优劣。 随着场景不断的深入，我们可以通过如下三种 标准 来评判模型的优劣： 针对单个样本，我们可以通过 损失函数最小 作为评判标准； 针对样本集，我们可以通过 经验风险最小 作为评判标准； 为规避 过拟合现象，我们可以通过 结构风险最小 作为评判标准； 2.1. 损失函数(loss function) 对于输入 XXX，假设我们选择模型 fff 作为决策函数，则预测值 f(X)f(X)f(X) 与真实值 YYY 可能一致。因此，通过损失函数度量预测错误的程度，记为 L(Y,f(X))L(Y, f(X))L(Y,f(X))。 例如回归模型中，损失函数经常为 L(Y,f(X))=(Y−f(X))2L(Y, f(X)) = (Y-f(X))^2L(Y,f(X))=(Y−f(X))​2​​ 。损失函数值越小，模型越好。 2.2. 经验风险(empirical risk) 一般而言，我们使用的训练集都是总体的样本集。当样本集足够大时，样本集中所有输入的损失函数值的均值，可以代替总体的损失函数值的期望，作为衡量模型优劣的标准，我们称之为经验风险： Rer(f)=1N∑i=1NL(yi,f(xi)) \\qquad R_{er}(f) = \\frac{1}{N} \\sum_{i=1}^N L(y_i,f(x_i)) R​er​​(f)=​N​​1​​​i=1​∑​N​​L(y​i​​,f(x​i​​)) 因此，可以将经验风险最小化用来求最优模型。 “极大似然估计” 就是 经验风险最小化 的一个例子。 2.3. 结构风险(structural risk) 经验风险最小化的前提，是样本集足够大，即样本集的特征分布足以代替总体的特征分布。而当样本集较小时，此条件不足以满足，如果依然以经验风险最小化进行，就有可能出现过 “拟合现象”。为了解决此问题，我们提出了结构风险的概念。 过拟合是指模型包含的参数过多，出现模型对训练集拟合很好，但对未知数据预测很差的现象。 结构风险，就是在经验风险上加上表示模型复杂度的正则化项（惩罚项）： Rsr(f)=1N∑i=1NL(yi,f(xi))+λJ(f) \\qquad R_{sr}(f) = \\frac{1}{N} \\sum_{i=1}^N L(y_i,f(x_i))+\\lambda J(f) R​sr​​(f)=​N​​1​​​i=1​∑​N​​L(y​i​​,f(x​i​​))+λJ(f) 其中， J(f)J(f)J(f) 表征模型的复杂度，模型越复杂 J(f)J(f)J(f) 越大。 λ≥0\\lambda \\geq 0λ≥0 是系数，用以权衡经验风险和模型复杂度。 一般来说，针对小样本集来说，经验风险最小仅能保证训练集效果好，而结构风险最小能保证训练和测试集都有较好的预测。 3. 学习算法 确定机器学习的模型集合、模型的评价策略后，就需要通过 学习算法 来求解最优化问题，以确定模型集合中哪个模型最符合模型的评价策略。 "},"Content/Machine-Learning/Technology/梯度下降.html":{"url":"Content/Machine-Learning/Technology/梯度下降.html","title":"II - 梯度下降(Gradient Descent)","keywords":"","body":" 梯度下降(Gradient Descent) 1. 最优化问题 2. 梯度下降(Gradient Descent) 2.1. 梯度 2.2. 梯度下降 2.3. 迭代的意义 2.4. 结束点 3. 两个问题 3.1. 局部极值 3.2. 鞍点(Saddle Point)问题 4. 批量梯度下降(Batch Gradient Descent) 5. 随机梯度下降(Stochastic Gradient Descent) 6. 小批量梯度下降法(Mini-Batch Gradient Descent) 梯度下降(Gradient Descent) see 梯度下降 1. 最优化问题 最优化问题是求解函数极值的问题，包括极大值和极小值。 为统一分析，我们一般把求极大值转化为求极小值，如 max(f(x))=min(−f(x)). \\max \\big( f(x) \\big) = \\min \\big( -f(x) \\big).max(f(x))=min(−f(x)). 一般来说，我们是通过微积分，对函数求导数为0的点。因为 导数为0 是 极值点 的必要条件（非充分条件）。 需要注意，一个函数可能有多个局部极值点，我们需要通过比较这些局部极值点才能找到全局极值点。 2. 梯度下降(Gradient Descent) 但是，当函数特别复杂时，求导为零很困难。例如： f(x,y)=x3−2x2+exy−y3+10y2+100sin(xy) f(x,y) = x^3 - 2x^2 + e^{xy} - y^3 + 10y^2 + 100 \\sin (xy) f(x,y)=x​3​​−2x​2​​+e​xy​​−y​3​​+10y​2​​+100sin(xy) 这个函数的求导为零就很难求解。由此，引入了 梯度下降 方法。 2.1. 梯度 梯度 是 多元函数 对各个自变量偏导数形成的向量，定义为： ∇f(x1,x2,...,xk)=(∂f∂x1,∂f∂x2,...,∂f∂xk)T \\qquad \\nabla f(x_1,x_2,...,x_k) = \\bigg( \\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},...,\\frac{\\partial f}{\\partial x_k} \\bigg)^T ∇f(x​1​​,x​2​​,...,x​k​​)=(​∂x​1​​​​∂f​​,​∂x​2​​​​∂f​​,...,​∂x​k​​​​∂f​​)​T​​ 其中 ∇ \\nabla ∇ 称为梯度算子，它作用于一个多元函数，得到一个向量。 例如 ∇(x2+xy−y2)=(2x+y,x−2y) \\nabla(x^2+xy-y^2) = (2x+y,x-2y) ∇(x​2​​+xy−y​2​​)=(2x+y,x−2y)。 可导函数在某一点处取得极值的必要条件是梯度为零，梯度为零的点称为函数的驻点，这是疑似极值点。需要注意，梯度为零只是函数取极值的必要条件而不是充分条件，即梯度为零的点可能不是极值点。如果需要确定是否极值点，还需要观察二阶导数： 如果二阶导数大于0，函数有极小值； 如果二阶导数小于0，函数有极大值； 如果二阶导数等于0，情况不定。 2.2. 梯度下降 对于复杂的函数，我们可以通过 迭代法，从一个初始点 x0 x_0 x​0​​，反复使用某种规则从 xk x_{k} x​k​​ 移动到下一个点 xk+1 x_{k+1} x​k+1​​，即 xk+1=h(xk) \\qquad x_{k+1} = h(x_{k}) x​k+1​​=h(x​k​​) 那么，具体的迭代规则是什么呢？ 我们将多元函数 f(x) f(x) f(x) 在 x0 x_0 x​0​​ 点处泰勒展开，有： f(x0+Δx)=f(x0)+(∇f(x0))TΔx+o(Δx) \\qquad f(x_0 + \\Delta x) = f(x_0) + \\big( \\nabla f(x_0) \\big)^T \\Delta x + \\omicron ( \\Delta x ) f(x​0​​+Δx)=f(x​0​​)+(∇f(x​0​​))​T​​Δx+o(Δx) 当 Δx \\Delta x Δx 足够小时，我们可以忽略高次项 o(Δx) \\omicron ( \\Delta x ) o(Δx)，可以得到： f(x0+Δx)−f(x0)≈(∇f(x))TΔx \\qquad f(x_0 + \\Delta x) - f(x_0) \\approx \\big( \\nabla f(x) \\big)^T \\Delta x f(x​0​​+Δx)−f(x​0​​)≈(∇f(x))​T​​Δx 假设我们现在要求 极小值（求极大值可以转化为求极小值），则需要令函数递减： f(x0+Δx)−f(x0)≈(∇f(x))TΔx⩽0 \\qquad f(x_0 + \\Delta x) - f(x_0) \\approx \\big( \\nabla f(x) \\big)^T \\Delta x \\leqslant 0 f(x​0​​+Δx)−f(x​0​​)≈(∇f(x))​T​​Δx⩽0 即 (∇f(x))TΔx=∣∣(∇f(x))T∣∣⋅∣∣Δx∣∣⋅cosθ⩽0 \\qquad \\big( \\nabla f(x) \\big)^T \\Delta x = || \\big( \\nabla f(x) \\big)^T || \\cdot || \\Delta x || \\cdot \\cos \\theta \\leqslant 0 (∇f(x))​T​​Δx=∣∣(∇f(x))​T​​∣∣⋅∣∣Δx∣∣⋅cosθ⩽0 其中，θ \\theta θ 是向量 (∇f(x))T \\big( \\nabla f(x) \\big)^T (∇f(x))​T​​ 和 Δx \\Delta x Δx 的夹角。 那么，只要 cosθ⩽0 \\cos \\theta \\leqslant 0 cosθ⩽0 即可。 特别的，在 θ=π \\theta = \\pi θ=π，即向量 Δx \\Delta x Δx 与梯度 (∇f(x))T \\big( \\nabla f(x) \\big)^T (∇f(x))​T​​ 反向时，cosθ=−1 \\cos \\theta = -1 cosθ=−1，就可以更快的朝极小值移动。因此，我们令 Δx=−η⋅(∇f(x))T \\Delta x = - \\eta \\cdot \\big( \\nabla f(x) \\big)^T Δx=−η⋅(∇f(x))​T​​ 则必有 cosθ=−1 \\cos \\theta = -1 cosθ=−1。其中，(∇f(x))T \\big( \\nabla f(x) \\big)^T (∇f(x))​T​​ 就是梯度，η \\eta η 我们称之为 步长，以此控制 Δx \\Delta x Δx 足够小。 2.3. 迭代的意义 梯度下降，表面上是迭代变量，每一次不断的修改变量。 实际上，迭代的意义是迭代梯度，不断的修正梯度。 2.4. 结束点 梯度下降 的方法就是不断的迭代，那么，迭代到什么时候停止呢？ 理想情况下，迭代到 梯度 为零的点即为极值点。 实际应用时，我们有两种方案： 可以指定一个阈值，更新后的梯度在阈值范围内即停止； 指定迭代次数，一旦达到迭代次数，无论是否找到极值均结束； 3. 两个问题 3.1. 局部极值 如果 函数 存在多个极值点，梯度下降方法可能找到的是局部极值点，而不是全局极值点。 一般情况下，我们可以选取多个随机点作为起始点，多计算几次，最终比较出哪个是全局极值点。 3.2. 鞍点(Saddle Point)问题 鞍点的数学含义是：目标函数在此点上的梯度（一阶导数）值为零， 但从该点出发的一个方向是函数的极大值点，而在另一个方向是函数的极小值点。 我们考虑如下函数： f(x,y)=x2−y2 \\qquad f(x,y) = x^2 - y^2 f(x,y)=x​2​​−y​2​​ 显然点 (0,0)(0,0)(0,0) 既不是极大值又不是极小值，但是其梯度在此点为零，算法会结束。这个点就称为 鞍点。 4. 批量梯度下降(Batch Gradient Descent) 机器学习中，一般是求样本集的损失函数的极小值，我们就通过 梯度下降方法 来求解。 假设样本集为 S={x1,x2,...,xN} S = \\{x_1,x_2,...,x_N\\} S={x​1​​,x​2​​,...,x​N​​}，单个样本的损失函数为 L(w,xi) L(w,x_i) L(w,x​i​​)，那么总的损失函数为： L(w)=∑i=1NL(w,xi) \\qquad L(w) = \\sum_{i=1}^N L(w,x_i) L(w)=​i=1​∑​N​​L(w,x​i​​) 其中，w w w 为我们需要求解的参数向量。 梯度下降 算法步骤： 对参数向量 www 取一个随机初始值 w0w_0w​0​​； 对 L(w) L(w) L(w) 参数向量 w w w 的每个分量求微分，代入 SSS 和 w0w_0w​0​​，求得梯度 d0 d_0 d​0​​； 更新参数向量 www：w1=w0−η⋅w0w_1 = w_0 - \\eta \\cdot w_0w​1​​=w​0​​−η⋅w​0​​； 重复步骤2、步骤3，一直到梯度为零； 以上的方法，我们称为 批量梯度下降。“批量”的意思，是指每一步迭代都会代入样本集 SSS 中的所有样本。 很明显，当样本集很大时，每一轮迭代都全量计算，消耗的 计算资源 和 时间 都比较大。因此，我们引入了 随机梯度下降。 5. 随机梯度下降(Stochastic Gradient Descent) 批量梯度下降（BGD） 的问题是每一轮迭代都需要计算全量样本集，目的是为了获取全局梯度，使得每一轮迭代都能比较准确的移动。 但是，并不是每一轮迭代都需要考虑所有样本点，尤其是在微调阶段，绝大部分样本点都是无用的。因此，我们极端的处理：每一轮迭代仅计算单个样本点，单个样本点达到梯度为零后再计算下一样本点，不断迭代到所有样本点，这就是 随机梯度下降（SGD）。 收敛性 通过之前的分析，我们可以理解 BGD 是肯定收敛到极小值的，那么 SGD 是否收敛呢？答案是肯定的，具体可参考 为什么随机梯度下降方法能够收敛？ 优缺点 相对于 BGD，SGD 可能会增加迭代的轮数，但每一轮迭代的计算量大大的降低了，总体来看效率高很多，这是最大的优点。 但是，由于每一轮迭代仅考虑单个样本点，而此样本点计算的梯度不一定是朝着极小化的方向移动，因此可能使得整个迭代过程出现反复，即噪音较多。但总体移动方向肯定是朝着极小化的方向移动的。 为解决 SGD 的噪音问题，引入了 小批量梯度下降法。 6. 小批量梯度下降法(Mini-Batch Gradient Descent) BGD 取全量样本集会导致训练慢，SGD 取单个样本点会导致噪音大，因此两者结合，每轮迭代从样本集中随机取部分样本点形成一个小数据量的训练集，就可以同时规避以上两个的问题，这就是 小批量梯度下降法（MBGD）。 "},"Content/Machine-Learning/Preprocessing/":{"url":"Content/Machine-Learning/Preprocessing/","title":"4.2. Preprocessing","keywords":"","body":"2.3.6. Preprocessing "},"Content/Machine-Learning/Preprocessing/Feature-Extraction/":{"url":"Content/Machine-Learning/Preprocessing/Feature-Extraction/","title":"4.2.1. Feature Extraction","keywords":"","body":"Feature Extraction "},"Content/Machine-Learning/Preprocessing/Feature-Extraction/特征转化.html":{"url":"Content/Machine-Learning/Preprocessing/Feature-Extraction/特征转化.html","title":"I - 特征转化(Feature Convert)","keywords":"","body":" 特征转化(Feature Convert) 1. 连续特征 2. 无序类别特征 3. 有序类别特征 特征转化(Feature Convert) 特征处理（Feature Processing） 特征转化，是指将特征值由 便于逻辑理解 的存储格式转化为 便于机器学习 的存储格式。 特征 通常属于如下三类： 连续（continuous）特征 无序类别（categorical）特征 有序类别（ordinal）特征 1. 连续特征 对于 连续 特征，其存储格式无论是 逻辑理解 还是 机器学习，使用的都比较一致，因此一般不需要进行什么转化（我们将 归一化 处理归为预处理，不算作特征转化）。 2. 无序类别特征 无序特征，例如特征 color，有三种取值：red、green、blue，这就是我们逻辑上对特征进行描述的格式。在常规数据处理中，为了便于存储及处理，我们常常会将 字符串 与 数字 一一映射，即使得 red=1、green=2、blue=3。 但是，这种转换方式会使得各取值之间可比较大小、甚至可进行运算，例如 1 + 2 = 3 会使得 red + green = blue。但实际上它们之间是不存在这种运算关系的。基于以上原因，在机器学习中，对无序特征的转化一般使用 One-hot（One-of-k）将取值转化为一个数值向量： color vector red (1,0,0)(1,0,0)(1,0,0) green (0,1,0)(0,1,0)(0,1,0) blue (0,0,1)(0,0,1)(0,0,1) 这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。 3. 有序类别特征 对于有序特征，取值之间存在一定的大小关系，但不存在运算关系，因此通过如下方式表示： status vector bad (1,0,0)(1,0,0)(1,0,0) normal (1,1,0)(1,1,0)(1,1,0) good (1,1,1)(1,1,1)(1,1,1) 这样就利用递进表达了值之间的顺序关系。 "},"Content/Machine-Learning/Preprocessing/Feature-Extraction/特征哈希.html":{"url":"Content/Machine-Learning/Preprocessing/Feature-Extraction/特征哈希.html","title":"II - 特征哈希(Feature Hashing)","keywords":"","body":" 特征哈希(Feature Hashing) 特征哈希(Feature Hashing) 特征哈希（Feature Hashing) 在某些场景，特征向量的维度会很大，这会导致存储量、计算量很大，因此我们需要降维。常见的降维方式有 聚类、PCA 等。但这些方法在特征量和样本量很多的时候本身就计算量很大，所以对大问题也基本无能为力。因此，出现了 特征哈希 这种较为直观和简单的降维方法。 特征哈希 的目标是把原始的高维特征向量压缩成较低维特征向量，且尽量不损失原始特征的表达能力。 假设哈希前的特征向量为 x⃗=(x1,x2,...,xN)∈RN \\vec{x} = (x_1, x_2, ... , x_N) \\in R^N ​x​⃗​​=(x​1​​,x​2​​,...,x​N​​)∈R​N​​。我们要把这个原始的 N N N 维特征向量压缩成 M M M 维（MN M MN），即 y⃗=(y1,y2,...,yM)∈RM \\vec{y} = (y_1, y_2, ... , y_M) \\in R^M ​y​⃗​​=(y​1​​,y​2​​,...,y​M​​)∈R​M​​。构造两个哈希函数： h(n):{1,2,...,N}→{1,2,...,M} h(n):\\{1,2,...,N\\} \\to \\{1,2,...,M\\} h(n):{1,2,...,N}→{1,2,...,M} g(n):{1,2,...,N}→{−1,1} g(n):\\{1,2,...,N\\} \\to \\{-1,1\\} g(n):{1,2,...,N}→{−1,1} h(n) h(n) h(n) 和 g(n) g(n) g(n) 是独立不相关的(这两个函数的入参表示特征向量的第 n n n 维)。 那么，我们令： yi=∑j:h(j)=ig(j)⋅xj y_i = \\sum_{j:h(j)=i} g(j) \\cdot x_j y​i​​=​j:h(j)=i​∑​​g(j)⋅x​j​​ 简单理解，就是把 HASH 到一起的两个维度带符号相加。 可以证明，按上面的方式生成的新特征 y⃗ \\vec{y}​y​⃗​​ 在概率意义下保留了原始特征空间的内积，以及距离： x⃗1T⋅x⃗2≈y⃗1T⋅y⃗2 \\vec{x}_1^T \\cdot \\vec{x}_2 \\approx \\vec{y}_1^T \\cdot \\vec{y}_2 ​x​⃗​​​1​T​​⋅​x​⃗​​​2​​≈​y​⃗​​​1​T​​⋅​y​⃗​​​2​​ ∣∣x⃗1−x⃗2∣∣≈∣∣y⃗1−y⃗2∣∣ ||\\vec{x}_1 - \\vec{x}_2|| \\approx ||\\vec{y}_1 - \\vec{y}_2|| ∣∣​x​⃗​​​1​​−​x​⃗​​​2​​∣∣≈∣∣​y​⃗​​​1​​−​y​⃗​​​2​​∣∣ 其中 x⃗1,x⃗2 \\vec{x}_1, \\vec{x}_2 ​x​⃗​​​1​​,​x​⃗​​​2​​ 为两个原始特征向量，而 y⃗1,y⃗2 \\vec{y}_1, \\vec{y}_2 ​y​⃗​​​1​​,​y​⃗​​​2​​为对应的哈希后的特征向量。 "},"Content/Machine-Learning/Preprocessing/Feature-Extraction/文本特征提取.html":{"url":"Content/Machine-Learning/Preprocessing/Feature-Extraction/文本特征提取.html","title":"III * 文本特征提取(Text Feature Extraction)","keywords":"","body":"III - 文本特征提取(Text Feature Extraction) "},"Content/Machine-Learning/Preprocessing/Feature-Extraction/图片特征提取.html":{"url":"Content/Machine-Learning/Preprocessing/Feature-Extraction/图片特征提取.html","title":"IV * 图片特征提取(Image Feature Extraction)","keywords":"","body":"IV - 图片特征提取(Image Feature Extraction) "},"Content/Machine-Learning/Preprocessing/Feature-Extraction/稀疏矩阵存储.html":{"url":"Content/Machine-Learning/Preprocessing/Feature-Extraction/稀疏矩阵存储.html","title":"V - 稀疏矩阵存储（CSR）","keywords":"","body":" 稀疏矩阵存储（CSR） 1. COO: Coordinate 2. CSR: Compressed Sparse Row 稀疏矩阵存储（CSR） 稀疏矩阵是指矩阵中的元素大部分是0的矩阵。事实上，实际问题中大规模矩阵基本上都是稀疏矩阵，很多稀疏度在90%甚至99%以上。因此我们需要有高效的稀疏矩阵存储格式。这里主要讲两种格式：COO 和 CSR。 1. COO: Coordinate COO 是最简单的一种格式，每一个元素需要用一个三元组来表示，分别是（行号，列号，数值），对应下图右边的一列。 这种方式最简单直观，但是记录单信息多（行列），每个三元组自己可以定位，因此空间不是最优。 2. CSR: Compressed Sparse Row CSR是比较标准的一种，也是 sklearn 包使用的格式，如下图所示： CSR 也需要三类数据来表达：数值，列号，以及行偏移。 注意，行偏移量并不是与所有非0元素一一对应的，因此，CSR不是三元组，而是整体的编码方式。 CSR 的数值和列号与 COO 一致，表示一个元素以及其列号，行偏移表示某一行的第一个非0元素在values数组里面的起始偏移位置。因此，有多少行，就会有多少行偏移量。 如上图中： 第一行的首非0元素1在values数组中是0偏移，因此 行偏移数组第一位是0； 第二行的首非0元素2在values数组中是2偏移，因此 行偏移数组第二位是2； 第三行的首非0元素5在values数组中是4偏移，因此 行偏移数组第三位是4； 第四行的首非0元素6在values数组中是7偏移，因此 行偏移数组第四位是7； 最后，在 行偏移数组 补上矩阵总的非0元素个数。 "},"Content/Machine-Learning/Preprocessing/Preprocessing/":{"url":"Content/Machine-Learning/Preprocessing/Preprocessing/","title":"4.2.2. Preprocessing","keywords":"","body":"2.3.2.1. Preprocessing "},"Content/Machine-Learning/Preprocessing/Preprocessing/特征缩放.html":{"url":"Content/Machine-Learning/Preprocessing/Preprocessing/特征缩放.html","title":"I - 特征缩放(Feature Scaling)","keywords":"","body":" 特征缩放(Feature Scaling) 1. 归一化(Normalization) 1.1. 不同样本同一特征 1.2. 同一样本不同特征 2. 中心化(Centered) 3. 标准化(Standardization) 4. 归一化 vs 标准化 特征缩放(Feature Scaling) see Feature scaling 特征缩放 是在数据预处理阶段进行的数据标准化的方法。 这是由于，原始数据的值范围往往变化很大，在一些算法中，如果没有标准化，可能效果会很差，例如： 我们经常使用 欧式距离 计算两点之间的距离。但如果点向量中某项要素变化范围很大，那么距离就会严重依赖此项要素的值； 梯度下降方法中，特征缩放会使得算法收敛效率快的多； 特征缩放 根据 目的不同，大概分为 归一化、中心化、标准化 三类。 1. 归一化(Normalization) 归一化 的目的，是将数据的范围限定在一个较小的范围（如 [0,1][0,1][0,1]），并且消除量纲的影响。 假设样本集 S={x⃗1,x⃗2,...,x⃗N} S = \\{\\vec{x}_1,\\vec{x}_2,...,\\vec{x}_N\\} S={​x​⃗​​​1​​,​x​⃗​​​2​​,...,​x​⃗​​​N​​}，其中样本 x⃗i=(xi,1,xi,2,...,xi,k) \\vec{x}_i = (x_{i,1},x_{i,2},...,x_{i,k}) ​x​⃗​​​i​​=(x​i,1​​,x​i,2​​,...,x​i,k​​) 是表示 k k k 个特征项的 k k k 维向量。 那么，根据视角的不同，可以将 归一化 分为两种： 1.1. 不同样本同一特征 基于样本集的某项特征进行归一化，而不考虑各个特征之间的关系： Rescaling (min-max normalization) xi,j′=xi,j−min(x∗,j)max(x∗,j)−min(x∗,j) x_{i,j}' = \\frac{x_{i,j} - \\min(x_{*,j})}{\\max(x_{*,j}) - \\min(x_{*,j})} x​i,j​′​​=​max(x​∗,j​​)−min(x​∗,j​​)​​x​i,j​​−min(x​∗,j​​)​​ Mean normalization xi,j′=xi,j−mean(x∗,j)max(x∗,j)−min(x∗,j) x_{i,j}' = \\frac{x_{i,j} - mean(x_{*,j})}{\\max(x_{*,j}) - \\min(x_{*,j})} x​i,j​′​​=​max(x​∗,j​​)−min(x​∗,j​​)​​x​i,j​​−mean(x​∗,j​​)​​ 此类型方法，当有新数据加入时，可能使得 mean mean mean、max \\max max、min \\min min 发生变化，进而导致所有的样本数据均需要重新计算。 1.2. 同一样本不同特征 此种方法在单个样本的各个特征之间进行归一化，而不考虑各个样本之间的关系： Scaling to unit length xi,j′=xi,j∣∣x⃗i∣∣=xi,jxi,12+xi,22+...+xi,k2 x_{i,j}' = \\frac{x_{i,j}}{||\\vec{x}_i||} = \\frac{x_{i,j}}{\\sqrt{x_{i,1}^2 + x_{i,2}^2 + ... + x_{i,k}^2}} x​i,j​′​​=​∣∣​x​⃗​​​i​​∣∣​​x​i,j​​​​=​√​x​i,1​2​​+x​i,2​2​​+...+x​i,k​2​​​​​​​x​i,j​​​​ 此方法可使得所有样本的特征向量的长度均固定为1，更便于样本间进行分析。 2. 中心化(Centered) 中心化 的目的，是将样本集中某项特征值的均值转化为零，这样，这些特征值就会分布在零点周围。 xi,j′=xi,j−mean(x∗,j) x_{i,j}' = x_{i,j} - mean(x_{*,j}) x​i,j​′​​=x​i,j​​−mean(x​∗,j​​) 中心化 依然是基于样本集的某项特征进行缩放，而不考虑各个特征之间的关系。 3. 标准化(Standardization) 一般来说，我们都假设特征值的分布是正态分布。因此，标准化 的目的是将样本集中某项特征值的正态分布转化为 标准正态分布，即 均值为0、方差为1的正态分布。 xi,j′=xi,j−mean(x∗,j)σj x_{i,j}' = \\frac{x_{i,j} - mean(x_{*,j})}{\\sigma_j} x​i,j​′​​=​σ​j​​​​x​i,j​​−mean(x​∗,j​​)​​ 标准化 依然是基于样本集的某项特征进行缩放，而不考虑各个特征之间的关系。 4. 归一化 vs 标准化 同： 归一化 和 标准化 均会将特征值缩放； 归一化 和 标准化 均不会影响单项特征值的分布； 异： 归一化 将特征值缩放到 [0,1] [0,1] [0,1] 的范围内，而 标准化 将特征值缩放到 0 0 0 周围； 归一化 会将所有特征值均缩放到 [0,1] [0,1] [0,1] ，虽不影响单项特征值的分布，但会影响各项特征值之间的比较。 而 标准化 除保留单项特征值的分布外，也会保留各项特征值之间的比较； 例如下例中，由于量纲的影响，原始数据表征为椭圆形，而经过归一化处理后，会变化为圆形。而 标准化 处理后图形依然会表征为椭圆形； "},"Content/Machine-Learning/Dimensionality-Reduction/":{"url":"Content/Machine-Learning/Dimensionality-Reduction/","title":"4.3. Dimensionality Reduction","keywords":"","body":"2.3.5. Dimensionality Reduction "},"Content/Machine-Learning/Model-Selection/":{"url":"Content/Machine-Learning/Model-Selection/","title":"4.4. Model Selection","keywords":"","body":"2.3.4. Model Selection "},"Content/Machine-Learning/Model-Selection/Grid-Search/":{"url":"Content/Machine-Learning/Model-Selection/Grid-Search/","title":"4.4.1. Grid Search","keywords":"","body":"2.3.4.1. Grid Search "},"Content/Machine-Learning/Model-Selection/Cross-Validation/":{"url":"Content/Machine-Learning/Model-Selection/Cross-Validation/","title":"4.4.2. Cross Validation","keywords":"","body":"2.3.4.2. Cross Validation "},"Content/Machine-Learning/Model-Selection/Metrics/":{"url":"Content/Machine-Learning/Model-Selection/Metrics/","title":"4.4.3. Metrics","keywords":"","body":"2.3.4.3. Metrics "},"Content/Machine-Learning/Classification/":{"url":"Content/Machine-Learning/Classification/","title":"4.5. Classification","keywords":"","body":"2.3.2. Classification "},"Content/Machine-Learning/Classification/分类模型的评估.html":{"url":"Content/Machine-Learning/Classification/分类模型的评估.html","title":"I - 分类模型的评估","keywords":"","body":" 分类模型的评估 1. 预测结果的分类 2. 分类的判别阈值 3. Accuracy 4. Precision & Recall & F-measure 4.1. Precision 4.2. Recall 4.3. F-measure 4.4. Summary 5. PRC & AP & MAP 5.1. PRC(Precision Recall Curve) 5.2. AP(Average Precision) 5.3. MAP(Mean Average Precision) 6. ROC & AUC 6.1. TPR & FPR 6.2. ROC(Receiver Operating Characteristic) 6.3. AUC(Area Under ROC Curve) 7. ROC vs PRC 7.1. ROC 与 PRC 的评估结果一致 7.2. PRC右上角最优 vs TOC左上角最优 7.3. ROC 比 PRC 更平滑 7.4. ROC 不一定比 PRC 更准确 8. Demo of Python 分类模型的评估 本文主要分析 分类模型 的评估标准。实际上，搜索模型也可以采用同样的评估标准，只需把每一次搜索作为一个分类即可。 本文以 N N N 分类举例，样本集合为 S S S，分类类别集合为 C={ci,c2,...cn−1,cn} C=\\{c_i, c_2, ... c_{n-1}, c_n\\} C={c​i​​,c​2​​,...c​n−1​​,c​n​​} 1. 预测结果的分类 对于所有分类模型（不仅仅是二分类），其中每一个分类类别的预测结果均可以分为四类： TP —— True Positive FP —— Flase Positive TN —— True Negative FN —— False Negative - 真实结果：cic_ic​i​​ 真实结果：not cinot \\space c_inot c​i​​ 预测结果：cic_ic​i​​ TPciTP_{c_i}TP​c​i​​​​ FPciFP_{c_i}FP​c​i​​​​ 预测结果：not cinot \\space c_inot c​i​​ FNciFN_{c_i}FN​c​i​​​​ TNciTN_{c_i}TN​c​i​​​​ 2. 分类的判别阈值 分类模型，主要包含两个部分： 分类算法 分类算法 基于样本的 业务指标 计算得到一个 分类指标（大多数算法是得到一个概率值） 判别阈值(Threshold) 分类指标 基于 判别阈值 对样本进行分类 也就是说，随着 判别阈值 的变化，样本分类也可能发生变化。同一分类算法，判别阈值 越高，TP,FPTP,FPTP,FP 越少，FN,TNFN,TNFN,TN 越多。 3. Accuracy 评估分类模型的预测结果，最直观的就是看预测的准确率，即 预测结果与真实结果相同的样本数量 在 总样本 中的占比，我们称之为 Accuracy【准确率】： Accuracy=∑i=1nTPci∣S∣ \\qquad Accuracy = \\frac{\\sum_{i = 1}^{n} TP_{c_i}}{|S|} Accuracy=​∣S∣​​∑​i=1​n​​TP​c​i​​​​​​ 一般来说，Accuracy 能直观的反映预测的情况。但是，我们很难量化的定义 Accuracy 的标准。也就是说，分类模型的 Accuracy 达到多少我们可以认为这个模型是优秀的？ 举个医疗领域的例子。假设某病症的发病率为 0.01%，如果分类模型对所有输入样本均返回“未发病”（即发病的判别阈值>1），那么 Accuracy 可达到 99.99%。但是，这么“高”的 Accuracy 没有任何意义。 上面的案例中，Accuracy 失效的原因之一，是由于其是针对所有分类类别统一分析准确率，这样会掩藏部分分类类别的“不准确”，尤其是 样本数量少但非常重要 的分类类别。由此，我们认识到需要对各个分类类别分开分析，这就是 Precision、Recall 和 F-measure。 4. Precision & Recall & F-measure 4.1. Precision Accuracy 是所有分类类别的预测结果的准确率，而 Precision【精确率/查准率】是指某分类类别的预测结果的准确率，例如 cic_ic​i​​ 的 Precision 为： Precisionci=TPciTPci+FPci \\qquad Precision_{c_i} = \\frac{TP_{c_i}}{TP_{c_i}+FP_{c_i}} Precision​c​i​​​​=​TP​c​i​​​​+FP​c​i​​​​​​TP​c​i​​​​​​ 一般来说，Precision 越高，此类别的分类效果越好。 但是，如果我们把 cic_ic​i​​ 的 分类判别阈值 定的很高，只在非常有把握的情况下才将样本判别为 cic_ic​i​​，这样可以明显提高 Precisionci Precision_{c_i} Precision​c​i​​​​。但是，这样我们会遗漏大量的真实分类为 cic_ic​i​​ 的样本。因此，引入了 Recall。 4.2. Recall Recall【召回率/查全率】，顾名思义，是指真实结果为某分类类别的所有样本被准确找到的比例。例如 cic_ic​i​​ 的 Recall 为： Recallci=TPciTPci+FNci \\qquad Recall_{c_i} = \\frac{TP_{c_i}}{TP_{c_i}+FN_{c_i}} Recall​c​i​​​​=​TP​c​i​​​​+FN​c​i​​​​​​TP​c​i​​​​​​ 一般来说，Recall 越高，此类别的分类效果越好。 但是，如果我们把 cic_ic​i​​ 的 分类判别阈值 定的很低，甚至所有样本均判定为 cic_ic​i​​，这样 RecallciRecall_{c_i}Recall​c​i​​​​ 可以达到很高。但实际上，这样并没有意义。 4.3. F-measure 理想情况下，我们希望 Precision 和 Recall 都很高。但实际场景中，两者一般是此消彼长的关系：随着判别阈值的提高，Precision 逐渐提高，Recall 逐渐降低。 因此，为了统一的评估分类模型，我们定义了 F-measure（也作F-score）： 2F_measure=1Precision+1Recall \\qquad \\frac{2}{F\\_measure} = \\frac{1}{Precision} + \\frac{1}{Recall} ​F_measure​​2​​=​Precision​​1​​+​Recall​​1​​ F_measure=2⋅Precision⋅RecallPrecision+Recall \\qquad F\\_measure = \\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall} F_measure=​Precision+Recall​​2⋅Precision⋅Recall​​ 需要注意，F-measure 依然是针对某一分类类别而定义的。 4.4. Summary 回到上面提到的病症案例。如果分类模型对所有输入样本均返回“未发病”（即发病的判别阈值>1），那么： 分类类别 Precision Recall F-measure 发病 0%0\\%0% 0%0\\%0% - 未发病 99%99\\%99% 100%100\\%100% 99.5%99.5\\%99.5% 对比 Accuracy，Precision + Recall + F-measure 能细化到对每一个分类类别进行分析，我们就可以选择更关注的类别进行分析。例如，病症案例中，我们更关注 “发病”类别的指标。而从“发病”类别的指标来看，案例中的分类模型就非常不好，符合我们的判断。 5. PRC & AP & MAP 5.1. PRC(Precision Recall Curve) Precision、Recall、F-measure 是 分类结果 的评估指标，而分类结果由 分类算法 和 判别阈值 共同决定。一般来说，分类算法相对比较固定，而 判别阈值 却可以有比较大的变化。因此，观察 判别阈值 的变化对各项评估指标的影响，可以帮助我们： 选择某分类模型中更为合理的 判别阈值； 更为全面的比较两个分类模型； 我们以 Precision 为纵轴，Recall 为横轴，将每个样本的值（如概率）作为判别阈值，多个样本形成多个PR点，进而形成 PRC 曲线。下图为两个分类模型的PRC曲线： 需要注意，PRC 曲线的自变量是 判别阈值，更准确的说是样本的分类指标。 我们分析PRC曲线的四个边界点： (0,0)：Recall 为 0，Precision 为 0，说明此分类模型特别不好，一个正确的分类都没有找到； (0,1)：Recall 为 0，Precision 为 1，说明此分类模型的 判定阈值 特别高，几乎全部判定为负样本，这样即使准确率高也不好； (1,0)：Recall 为 1，Precision 为 0，说明此分类模型的 判定阈值 特别低，几乎将所有样本都归为正样本，这样即使查全率高也不好； (1,1)：Recall 为 1，Precision 为 1，说明此分类模型非常好，分类又全又准； 因此： 单条PRC曲线中，越靠近右上角(1,1)的点的 判别阈值 对应的 评估指标 综合性更好（对于某些场景，更在意 Precision 和 Recall 中的一个，就不需要靠近右上角）； 多条PRC曲线中，越在上的曲线对应的 分类模型 更好，如上图中红色曲线更好。这是因为上方的曲线对同样的 Precision 有更高的 Recall，对同样的 Recall 有更高的 Precision； 另外，相对光滑的曲线更为稳定，不会因 判别阈值 的轻微变化导致 评估指标 剧烈抖动。 5.2. AP(Average Precision) 观察 PRC 曲线较为抽象，既然认为上方的曲线更好，那么很容易想到以 PRC 曲线下的面积作为指标进行判定，这就是 AP(Average Precision)，这里的 Average 相当于是对 Precision 取均值。 5.3. MAP(Mean Average Precision) 需要注意，Precision、Recall、F-measure、PRC、AP 都是针对单个分类类别的，如果我们需要分析整体分类的情况，就需要对所有类别求均值，即 MAPci=1∣S∣⋅∑i=1nAPci \\qquad MAP_{c_i} = \\frac{1}{|S|} \\cdot \\sum_{i=1}^{n} AP_{c_i} MAP​c​i​​​​=​∣S∣​​1​​⋅​i=1​∑​n​​AP​c​i​​​​ MAP仅在所有分类类别都同等重要的情况下使用，例如用作搜索模型的评估，每一个搜索关键字都可以作为一个分类类别，再通过MAP汇聚结果。而之前医药案例中我们有侧重的分类类别，就可以不关注MAP，否则可能导致误导。 6. ROC & AUC PRC 曲线相关的评估指标，是分类模型的一套评估体系。另外，还有 ROC 曲线相关的一套评估体系。我们先给出定义，然后比较两者的使用场景。 see ROC & AUC 6.1. TPR & FPR TPRTPRTPR : True Positive Rate，所有正样本中被预测为正的比例，也就是 Recall TPRci=TPciTPci+FNci=Recallci \\qquad TPR_{c_i} = \\frac{TP_{c_i}}{TP_{c_i} + FN_{c_i}} = Recall_{c_i} TPR​c​i​​​​=​TP​c​i​​​​+FN​c​i​​​​​​TP​c​i​​​​​​=Recall​c​i​​​​ FPRFPRFPR : False Positive Rate，所有负样本中被预测为正的比例 FPRci=FPciTNci+FPci \\qquad FPR_{c_i} = \\frac{FP_{c_i}}{TN_{c_i} + FP_{c_i}} FPR​c​i​​​​=​TN​c​i​​​​+FP​c​i​​​​​​FP​c​i​​​​​​ 6.2. ROC(Receiver Operating Characteristic) ROC 曲线以 TPR 为纵轴，FPR 为横轴，将每个样本的值（如概率）作为判别阈值，多个样本形成 ROC 曲线。下图为三个分类模型的 ROC 曲线： 与 PRC 曲线一样，ROC 曲线的自变量也是 判别阈值 我们分析 ROC 曲线的四个边界点： (0,0)：FPR 为 0，TPR 为 0，说明此分类模型的 判定阈值 特别高，全部判定为负样本； (0,1)：FPR 为 0，TPR 为 1，说明此分类模型非常好，分类又全又准； (1,0)：FPR 为 1，TPR 为 0，说明此分类模型特别不好，一个正确的分类都没有找到； (1,1)：FPR 为 1，TPR 为 1，说明此分类模型的 判定阈值 特别低，全部判定为正样本； 因此： 单条 ROC 曲线中，越靠近左上角(0,1)的点的 判别阈值 对应的 评估指标 综合性更好（对于某些场景，更在意 FPR 和 TPR 中的一个，就不需要靠近左上角）； 多条 ROC 曲线中，越在上的曲线对应的 分类模型 更好。这是因为上方的曲线对同样的 TPR 有更小的 FPR，对同样的 FPR 有更高的 TPR； 6.3. AUC(Area Under ROC Curve) 和 AP 类似，AUC 是指 ROC 曲线下的面积，显然这个面积的数值不会大于1。使用AUC 作为评价标准是因为很多时候 ROC 曲线并不能清晰的说明哪个分类模型的效果更好，而作为一个数值，AUC 更大的分类模型效果更好。 7. ROC vs PRC see ROC vs PRC 7.1. ROC 与 PRC 的评估结果一致 同一数据集，不同分类算法的 ROC 和 PRC 的优劣一致：如果A算法的 ROC 优于B算法，那么A算法的 PRC 也一定优于B算法，反之亦然。 see 《The Relationship Between Precision-Recall and ROC Curves》 那么，针对不同分类模型间进行比较时，逻辑上 ROC 和 PRC 的结果是一致的。 7.2. PRC右上角最优 vs TOC左上角最优 PRC 横轴是 Recall，纵轴是 Precision，这两个指标都是越大越好，因此右上角最优； TOC 横轴是 FPR，越小越好；纵轴是 TPR，也就是 Recall，越大越好。因此左上角最优； 7.3. ROC 比 PRC 更平滑 首先，我们看看同样的分类模型在不同的样本集中 ROC 和 PRC 的差异。下图中： (a)和(c)为 ROC 曲线，(b)和(d)为 PRC 曲线； (a)和(b)的样本集中正负样本数量比例为1：1； (c)和(d)的样本集中正负样本数量比例为1：10； 可以看出，在不同的样本集中： ROC 始终很平滑； PRC 有时很平滑，有时剧烈变化。 这是由于两个曲线不同的组成指标的公式决定的： TPR/Recall 的分母为“真实正样本的数量”，一旦样本集确定，就不再因 判别阈值 的变化而变化。分子为“预测准确的正样本的数量”，随着 判别阈值 的变化单调变化，比较平滑； FDR 的分母为“真实负样本的数量”，一旦样本集确定，就不再因 判别阈值 的变化而变化。分子为“预测错误的负样本的数量”，随着 判别阈值 的变化单调变化，比较平滑； Precision 的分母为“预测为正样本的数量”，分子为“预测准确的正样本的数量”，都会随着判别阈值 的变化而变化，因此 Precision 的变化整体不单调，变化比较剧烈。 因此，我们可以得出结论：在大部分样本集上，ROC 比 PRC 更平滑，这也是 ROC 更常用的原因之一。 但是，我们需要清楚的认识到，ROC 比 PRC 更平滑只是其变化趋势较缓，并不代表 TOC 比 PRC 更准确。 7.4. ROC 不一定比 PRC 更准确 下图为 同一样本集 的 ROC 和 PRC： ROC 很靠近（0,1）点，说明分类模型很好； PRC 离（1,1）点比较远，说明分类模型不太好； 两个曲线得到不同的结论，那么哪个更准确呢？ 两个曲线中的 红点，其实是同一个点，结合两张图我们可以得到： TPR=Recall=0.8,Precision=0.05,FPR=0.1 \\qquad TPR = Recall = 0.8 ,\\quad Precision = 0.05, \\quad FPR = 0.1 TPR=Recall=0.8,Precision=0.05,FPR=0.1 我们假设 真实正样本数量为 100，即 TP + FN = 100： 结合 TPR = 0.8，得到 TP = 80，FN = 20； 结合 Precision = 0.05，得到 FP = 1520； 结合 FPR = 0.1，得到 TN = 13680； 通俗的说，有 100 个正样本，15200 个负样本，这种情况下我们更看重将 正样本 分类正确的表现。而曲线对应的分类模型在 红点 处将 1520 + 80 = 1600 个样本判定为正样本，但其中只有 80 个正确。主观上还是认为这个分类模型有提升空间的。 那么，为什么 ROC 和 PRC 会有这个差异呢？这是由于 PRC 的 Precision、Recall 都是针对我们在意的分类类别在分析，例如上例就是仅分析 正样本 的分类表现，不在意 负样本 的分类表现。而 ROC 的 TPR、FPR 综合考虑了两种分类类别的分析。 因此，我们可以得出结论：ROC 不一定比 PRC 更准确。所以，我们最好两条曲线都进行观察，然后针对不同的场景进行选择。 8. Demo of Python 我们可以通过 sklearn 包计算：ROC —— sklearn.metrics.roc_curveAUC —— sklearn.metrics.aucPRC —— sklearn.metrics.precision_recall_curveAP —— sklearn.metrics.average_precision_score 参考官网的案例，我们可以画出 ROC 并计算 AUC import numpy as np from sklearn import metrics import matplotlib.pyplot as plt y = np.array([1, 1, 2, 2]) scores = np.array([0.1, 0.4, 0.35, 0.8]) fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2) auc = metrics.auc(fpr, tpr) print(\"fpr:\",fpr) print(\"tpr:\",tpr) print(\"thresholds:\",thresholds) print(auc) plt.plot(fpr,tpr,marker = 'o') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver operating characteristic example') plt.show() 执行结果如下： fpr: [0. 0.5 0.5 1. ]tpr: [0.5 0.5 1. 1. ]thresholds: [0.8 0.4 0.35 0.1 ]auc: 0.75 "},"Content/Machine-Learning/Classification/感知机.html":{"url":"Content/Machine-Learning/Classification/感知机.html","title":"II * 感知机(Perceptron)","keywords":"","body":" 感知机(Perceptron) 1. 模型集合 2. 目标策略 3. 学习算法 3.1. 原始形式 3.2. 对偶形式 4. 几何意义 5. Demo of Python 感知机(Perceptron) 感知机是二分类的线性分类模型，在特征空间中通过一个超平面将特征向量分为两类，是神经网络和支持向量机的基础。下面按照机器学习三要素（模型集合、目标策略、学习算法）对感知机进行分析。 see Perceptron 1. 模型集合 感知机是一种线性分类模型。对于分类模型，假设输入空间（特征空间）是 x∈Rn x \\in R^n x∈R​n​​，输出空间为 y∈{+1,−1} y \\in \\{+1,-1\\} y∈{+1,−1}，那么分类模型统一的公式为： y=Classification(x) y = Classification(x) y=Classification(x) 感知机模型，是通过超平面（线性模型）将所有特征向量进行分隔，在超平面两侧的特征向量分别为两类。我们知道，超平面 的表达式为： w⋅x+b=0 w \\cdot x + b = 0 w⋅x+b=0 其中，w⋅x w \\cdot x w⋅x 表示内积，w∈Rn w \\in R^n w∈R​n​​ 称作权值向量，b∈R b \\in R b∈R 称作偏置，均为模型参数。 在超平面两侧分别为两类，则分别为 w⋅x+b>0 w \\cdot x + b > 0 w⋅x+b>0 和 w⋅x+b0 w \\cdot x + b w⋅x+b0，那么可以使用符号函数 sign sign sign 作为分类函数： sign(t)={+1if t⩾0−1if t0 sign(t) = \\begin{cases} +1 &\\text{if } t\t\\geqslant 0 \\\\ -1 &\\text{if } t sign(t)={​+1​−1​​​if t⩾0​if t0​​ 由此得到 感知机分类模型 的完整公式： y=sign(w⋅x+b) y = sign(w \\cdot x + b) y=sign(w⋅x+b) 感知机的模型集合为特征空间中所有的线性分类模型，也可以理解为对应维度的所有超平面，我们根据 目标策略 和 学习算法 找出最合适的超平面即可。 2. 目标策略 按照感知机的分类目标，最直观的策略是误分类点的个数：误分类点越少感知机模型越好。但是，误分类点的个数不连续，更不可导，而损失函数我们一般希望其连续可导以便于计算。因此，替代将 所有误分类点到超平面的总距离 作为损失函数。 假设超平面为 w⋅x+b=0 w \\cdot x +b = 0 w⋅x+b=0，那么点 x0 x_0 x​0​​ 与此超平面的绝对距离为： L(w,b)x0=∣w⋅x0+b∣∣∣w∣∣ L(w,b)_{x_0} = \\frac{|w \\cdot x_0 +b|}{||w||} L(w,b)​x​0​​​​=​∣∣w∣∣​​∣w⋅x​0​​+b∣​​ 其中 ∣∣w∣∣ ||w|| ∣∣w∣∣ 表示 w w w 的 L2 L_2 L​2​​ 范数。在感知机中，我们仅需要判断正负及比较相对距离，因此可不除以 ∣∣w∣∣ ||w|| ∣∣w∣∣，取相对距离即可。 对于训练集 T={(x1,y1),(x2,y2),...(xN,yN)} T=\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\} T={(x​1​​,y​1​​),(x​2​​,y​2​​),...(x​N​​,y​N​​)}，其中 xi∈Rn x_i \\in R^n x​i​​∈R​n​​，yi∈{+1,−1} y_i \\in \\{+1,-1\\} y​i​​∈{+1,−1}。假设 M M M 为超平面误分类点的集合，则损失函数为： L(w,b)=−∑xi∈Myi(w⋅xi+b) L(w,b)=-\\sum_{x_i \\in M} y_i(w \\cdot x_i + b) L(w,b)=−​x​i​​∈M​∑​​y​i​​(w⋅x​i​​+b) 感知机的 目标策略 即求使得此损失函数最小的模型参数 w,b w,b w,b 。 3. 学习算法 按照常规的方法，求极值就是对损失函数的未知参数求导为零。但深入分析发现，感知机的损失函数有一个前提条件：误分类点的集合固定。而实际随着参数的变化，误分类点会发生变化，也就是说 误分类点集合不固定。因此，我们不能通过求导的方式来求极值。 此种情况下，比较适合使用 梯度下降 这种迭代的思路。 3.1. 原始形式 输入： 训练集 T={(x1,y1),(x2,y2),...(xN,yN)}T=\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\}T={(x​1​​,y​1​​),(x​2​​,y​2​​),...(x​N​​,y​N​​)}，其中 xi∈Rnx_i \\in R^nx​i​​∈R​n​​，yi∈{+1,−1}y_i \\in \\{+1,-1\\}y​i​​∈{+1,−1}； 学习率 η(1⩾η>0)\\eta(1 \\geqslant \\eta > 0)η(1⩾η>0)； 输出： 参数 w,b w, b w,b； 感知机模型 y=sign(w⋅x+b) y=sign(w \\cdot x+b) y=sign(w⋅x+b)； 算法： 随机选取初始值 w0,b0 w_0, b_0 w​0​​,b​0​​； 在训练集中选取数据 (xi,yi) (x_i, y_i) (x​i​​,y​i​​)； 如果 yi(w⋅xi+b)⩽0 y_i(w \\cdot x_i + b) \\leqslant 0 y​i​​(w⋅x​i​​+b)⩽0，我们需要使其大于0。将 f(w,b)=yi(w⋅xi+b) f(w,b) = y_i(w \\cdot x_i + b) f(w,b)=y​i​​(w⋅x​i​​+b) 视作函数，我们就通过调整参数 w,b w, b w,b，使得函数值不断增大。不同的是，这里的结束点仅需要使得函数值大于0，不需要梯度为零（实际上距离没有极大值，梯度也不可能为零）。因此我们更新参数： w=w+η⋅∂(yi(w⋅xi+b))∂w=w+η⋅yi⋅xi w = w + \\eta \\cdot \\frac{\\partial \\bigg( y_i(w \\cdot x_i + b)\\bigg)}{\\partial w} = w + \\eta \\cdot y_i \\cdot x_i w=w+η⋅​∂w​​∂(y​i​​(w⋅x​i​​+b))​​=w+η⋅y​i​​⋅x​i​​ b=b+η⋅∂(yi(w⋅xi+b))∂b=b+η⋅yi b = b + \\eta \\cdot \\frac{\\partial \\bigg( y_i(w \\cdot x_i + b)\\bigg)}{\\partial b} = b + \\eta \\cdot y_i b=b+η⋅​∂b​​∂(y​i​​(w⋅x​i​​+b))​​=b+η⋅y​i​​ 转至步骤2，直至训练集中没有误分类点。 3.2. 对偶形式 由于参数 w,b w,b w,b 每一轮迭代都会发生改变，导致每一轮都需要计算内积 w⋅xi w \\cdot x_i w⋅x​i​​，当特征维度较多时效率较低，所以考虑通过其他形式提升计算性能。 从原始形式的计算方式可以看出，如果初始点 w0=0w_0 = 0 w​0​​=0，则参数： w=∑i=1Nni⋅η⋅yi⋅xi=∑i=1Nαi⋅yi⋅xi w = \\sum_{i=1}^N n_i \\cdot \\eta \\cdot y_i \\cdot x_i = \\sum_{i=1}^N \\alpha_i \\cdot y_i \\cdot x_i w=​i=1​∑​N​​n​i​​⋅η⋅y​i​​⋅x​i​​=​i=1​∑​N​​α​i​​⋅y​i​​⋅x​i​​ b=∑i=1Nni⋅η⋅yi=∑i=1Nαi⋅yi b = \\sum_{i=1}^N n_i \\cdot \\eta \\cdot y_i = \\sum_{i=1}^N \\alpha_i \\cdot y_i b=​i=1​∑​N​​n​i​​⋅η⋅y​i​​=​i=1​∑​N​​α​i​​⋅y​i​​ 其中，ni n_i n​i​​ 表示某误分类点的修改次数，αi=ni⋅η \\alpha_i = n_i \\cdot \\eta α​i​​=n​i​​⋅η 作为参数。 可以看出，参数 w w w 可转化为样本点特征向量的 线性组合，可通过矩阵的方式提升计算效率。由此，得到原始形式的对偶形式。 输入： 训练集 T={(x1,y1),(x2,y2),...(xN,yN)}T=\\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\\}T={(x​1​​,y​1​​),(x​2​​,y​2​​),...(x​N​​,y​N​​)}，其中 xi∈Rnx_i \\in R^nx​i​​∈R​n​​，yi∈{+1,−1}y_i \\in \\{+1,-1\\}y​i​​∈{+1,−1}； 学习率 η(1⩾η>0)\\eta(1 \\geqslant \\eta > 0)η(1⩾η>0)； 输出： 参数 α,b \\alpha, b α,b，其中 α=(α1,α2...,αN)T \\alpha = (\\alpha_1,\\alpha_2...,\\alpha_N)^T α=(α​1​​,α​2​​...,α​N​​)​T​​； 感知机模型：y=sign(∑i=1Nαi⋅yi⋅xi⋅x+b) y=sign(\\sum_{i=1}^N \\alpha_i \\cdot y_i \\cdot x_i \\cdot x + b) y=sign(∑​i=1​N​​α​i​​⋅y​i​​⋅x​i​​⋅x+b)； 算法： 选取初始值 α=(0,0,...,0),b=0 \\alpha = (0,0,...,0), b = 0 α=(0,0,...,0),b=0； 在训练集中选取数据 (xj,yj) (x_j, y_j) (x​j​​,y​j​​)； 如果 yj(∑i=1Nαi⋅yi⋅xi⋅xj+b)⩽0 y_j(\\sum_{i=1}^N \\alpha_i \\cdot y_i \\cdot x_i \\cdot x_j+b) \\leqslant 0 y​j​​(∑​i=1​N​​α​i​​⋅y​i​​⋅x​i​​⋅x​j​​+b)⩽0，更新参数： αj=αj+η \\alpha_j = \\alpha_j + \\eta α​j​​=α​j​​+η b=b+η⋅yj b = b + \\eta \\cdot y_j b=b+η⋅y​j​​ 转至步骤2，直至训练集中没有误分类点。 可以看出，对偶形式中内积运算是固定的，我们可以预先把训练集中实例间的两两内积计算出来并以矩阵的形式存储，这个矩阵就是 Gram Gram Gram 矩阵。 G=[xi⋅xj]N×N G = [x_i \\cdot x_j]_{N \\times N} G=[x​i​​⋅x​j​​]​N×N​​ 基于 Gram Gram Gram 矩阵运算，可以提升计算效率。 see 感知机学习算法的对偶形式 4. 几何意义 我们以二维平面为例对几何意义进行说明。 训练集中的实例相当于平面中的点，感知机算法就相当于求一条直线，将正负两类分隔开。我们不妨假设此直线方程为： w⋅x+b=0 w \\cdot x + b = 0 w⋅x+b=0 其中，w w w 是直线的法向量，b b b 是原点到直线的截距（严格的说，应该称 w/∥w∥ w/\\|w\\| w/∥w∥ 为单位法向量，b/∥w∥ b/\\|w\\| b/∥w∥ 为原点到直线的物理截距）。 因此，在算法中 w=w+ηyixi w = w + \\eta y_i x_i w=w+ηy​i​​x​i​​ 就可以理解为将直线的法向量往误分类点 xi x_i x​i​​ 倾斜， b=b+ηyi b = b + \\eta y_i b=b+ηy​i​​ 就可以理解为将直线沿法向量往误分类点 xi x_i x​i​​ 移动。简单的说，就是一个调整方向，一个调整距离，最终使得直线越过该误分类点使其被正确分类。 5. Demo of Python import numpy as np from sklearn import metrics import matplotlib.pyplot as plt y = np.array([1, 1, 2, 2]) scores = np.array([0.1, 0.4, 0.35, 0.8]) fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2) auc = metrics.auc(fpr, tpr) print(\"fpr:\",fpr) print(\"tpr:\",tpr) print(\"thresholds:\",thresholds) print(auc) plt.plot(fpr,tpr,marker = 'o') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver operating characteristic example') plt.show() 执行结果如下： fpr: [0. 0.5 0.5 1. ]tpr: [0.5 0.5 1. 1. ]thresholds: [0.8 0.4 0.35 0.1 ]auc: 0.75 "},"Content/Machine-Learning/Regression/":{"url":"Content/Machine-Learning/Regression/","title":"4.6. Regression","keywords":"","body":"2.3.3. Regression "},"Content/Machine-Learning/Regression/线性回归.html":{"url":"Content/Machine-Learning/Regression/线性回归.html","title":"I * 线性回归(Linear Regression)","keywords":"","body":"线性回归(Linear Regression) LinearRegression 拟合一个带有系数 w = (w_1, ..., w_p) 的线性模型，使得数据集实际观测数据和预测数据（估计值）之间的残差平方和最小。其数学表达式为: minw∣∣Xw−y∣∣22 \\min_w {|| X w - y||_2}^2 ​w​min​​∣∣Xw−y∣∣​2​​​2​​ LinearRegression 会调用 fit 方法来拟合数组 X， y，并且将线性模型的系数 w 存储在其成员变量 coef_ 中。 然而，对于普通最小二乘（Ordinary Least Squares）的系数估计问题，其依赖于模型各项的相互独立性。当各项是相关的，且设计矩阵 X 的各列近似线性相关，那么，设计矩阵会趋向于奇异矩阵，这会导致最小二乘估计对于随机误差非常敏感，产生很大的方差。例如，在没有实验设计的情况下收集到的数据，这种多重共线性（multicollinearity）的情况可能真的会出现。 该方法使用 X 的奇异值分解来计算最小二乘解。如果 X 是一个 size 为 (n, p) 的矩阵，设 n \\geq p ，则该方法的复杂度为 O(n p^2) "},"Content/Machine-Learning/Clustering/":{"url":"Content/Machine-Learning/Clustering/","title":"4.7. Clustering","keywords":"","body":"2.3.4. Clustering "},"Content/Python/":{"url":"Content/Python/","title":"5. Python","keywords":"","body":"4.2 Python "},"Content/Python/Scikit-Learn/":{"url":"Content/Python/Scikit-Learn/","title":"5.1. Scikit-Learn v0.20.1","keywords":"","body":"Scikit-Learn v0.20.1 "},"Content/Python/Scikit-Learn/Datasets/":{"url":"Content/Python/Scikit-Learn/Datasets/","title":"5.1.1. Datasets","keywords":"","body":"Datasets sklearn.datasets Datasets 提供 三 种数据集接口：Loaders、Fetchers 和 Generations。 Loaders：加载 小数据量数据集，也称 Toy datasets； Fetchers：下载 并 加载 大数据量数据集，也称 Real world datasets； Generations：根据输入参数人为控制生成数据集； 他们都会返回： X: array[n_samples, n_features] y: array[n_samples] 对于 Loaders 和 Fetchers，还可以通过 DESCR 获取 特征列表。 "},"Content/Python/Scikit-Learn/Datasets/Loaders.html":{"url":"Content/Python/Scikit-Learn/Datasets/Loaders.html","title":"I - Loaders","keywords":"","body":" Loaders(Toy datasets) 1. Boston house prices 2. Iris plants 3. Diabetes 4. Optical recognition of handwritten digits dataset 5. Linnerrud 6. Wine recognition 7. Breast cancer Loaders(Toy datasets) 1. Boston house prices sklearn.datasets.load_boston 波士顿房屋价格的数据集，常用于 regression samples features 506 13 from sklearn.datasets import load_boston boston = load_boston() print(boston.data[0:5]) print(boston.target[0:5]) print(boston.feature_names) print(boston.DESCR) 2. Iris plants sklearn.datasets.load_iris 鸢尾花数据集，常用于 classification classes samples per class samples features 3 50 150 4 from sklearn.datasets import load_iris iris = load_iris() print(iris.data[0:5]) print(iris.target[0:5]) print(iris.feature_names) print(iris.DESCR) 3. Diabetes sklearn.datasets.load_diabetes 糖尿病数据集，常用于 regression samples features 442 10 from sklearn.datasets import load_diabetes diabetes = load_diabetes() print(diabetes.data[0:5]) print(diabetes.target[0:5]) print(diabetes.feature_names) print(diabetes.DESCR) 4. Optical recognition of handwritten digits dataset sklearn.datasets.load_digits 视觉识别手写数字的数据集，常用于 classification classes samples per class samples features 10 ~180 1797 64 from sklearn.datasets import load_digits digits = load_digits() print(digits.data[0:5]) print(digits.target[0:5]) print(digits.DESCR) 5. Linnerrud sklearn.datasets.load_linnerud 体能训练数据集。与其他数据集不同的是，此数据集的 y 不再是单列数据，而是 3 列数据，可以做 多项回归。 samples features of X features of y 20 3 3 from sklearn.datasets import load_linnerud linnerud = load_linnerud() print(linnerud.data) print(linnerud.target) print(linnerud.feature_names) print(linnerud.target_names) 6. Wine recognition sklearn.datasets.load_wine 酒类识别数据集，常用于 classification classes samples per class samples features 3 [59,71,48] 178 13 from sklearn.datasets import load_wine wine = load_wine() print(wine.data[0:5]) print(wine.target[0:5]) print(wine.DESCR) 7. Breast cancer sklearn.datasets.load_breast_cancer 乳腺癌数据集，常用于 binary classification classes samples per class samples features 2 212(M),357(B) 569 30 from sklearn.datasets import load_breast_cancer breast_cancer = load_breast_cancer() print(breast_cancer.data[0:5]) print(breast_cancer.target[0:5]) print(breast_cancer.DESCR) "},"Content/Python/Scikit-Learn/Datasets/Fetchers.html":{"url":"Content/Python/Scikit-Learn/Datasets/Fetchers.html","title":"II - Fetchers","keywords":"","body":" Fetchers(Real world datasets) 1. The Olivetti faces dataset 2. The 20 newsgroups text dataset 2.1. 20newsgroups 2.2. 20newsgroups vectorized 3. The Labeled Faces in the Wild face recognition 3.1. lfw people 3.2. lfw pairs 4. Forest covertypes 5. RCV1 dataset 6. Kddcup 99 dataset 7. California Housing dataset Fetchers(Real world datasets) 1. The Olivetti faces dataset sklearn.datasets.fetch_olivetti_faces 人脸识别的数据集，可以认为有 400 张图片，每张图片的像素均为 64*64=4096. classes samples features 40 400 4096 from sklearn.datasets import fetch_olivetti_faces olivetti_faces = fetch_olivetti_faces() print(olivetti_faces.data[0:1]) print(olivetti_faces.images[0:1]) print(olivetti_faces.target[0:1]) print(olivetti_faces.DESCR) 2. The 20 newsgroups text dataset 新闻数据集，将 18846 条新闻划分为 20 类。此数据集可通过 fetch_20newsgroups 和 fetch_20newsgroups_vectorized 两个函数获取，前者返回文本，后者返回特征向量。 2.1. 20newsgroups sklearn.datasets.fetch_20newsgroups classes samples features 20 18846 1 Parameters Data-Type Default Comment Note subset 'train', 'test', 'all' 'train' 加载的数据集 - categories None, collection of string, collection of unicode None 分类，即 classes - remove tuple () 特征提取时忽略的文本部分，('headers', 'footers', 'quotes')的子集 为避免过拟合，特征提取时经常会忽略 标题、页脚、引用 等 Returns Data-Type Comment Note bunch.data list[n_samples] 样本的新闻文本 - bunch.target list[n_samples] 样本的新闻种类 - bunch.target_names list[n_classes] 所有新闻种类的集合 - from sklearn.datasets import fetch_20newsgroups newsgroups = fetch_20newsgroups() print(newsgroups.data[0:2]) print(newsgroups.target[0:2]) print(newsgroups.target_names) print(len(newsgroups.target_names)) print(newsgroups.DESCR) 2.2. 20newsgroups vectorized sklearn.datasets.fetch_20newsgroups_vectorized 通过 sklearn.feature_extraction.text.CountVectorizer sklearn.feature_extraction.text.HashingVectorizer sklearn.feature_extraction.text.TfidfTransformer sklearn.feature_extraction.text.TfidfVectorizer 对 20newsgroups 文本处理后的数据集。 classes samples features 20 18846 130107 from sklearn.datasets import fetch_20newsgroups_vectorized newsgroups = fetch_20newsgroups_vectorized() print(newsgroups.data[0:2]) print(newsgroups.target[0:2]) print(newsgroups.target_names) print(len(newsgroups.target_names)) 3. The Labeled Faces in the Wild face recognition 采集的人脸的图片数据集，主要可用作 人脸验证（Face Verification）和 人脸识别（Face Recognition）。 人脸识别模型 Viola-Jones，实现类库 OpenCV。 3.1. lfw people sklearn.datasets.fetch_lfw_people classes samples features 5749 13233 5828 Parameters Data-Type Default Comment Note min_faces_per_person int None 数据集中的人物至少存在多少张图片 - color boolean False 是否保留 RGB 3个元素，如果 False 则会转化为 gray 1个元素 - slice_ slice slice(70,195,None), slice(78,172,None) 提供一个 slice(height, width) 的长方形以提取图片内容，避免提取过多的背景元素 - Returns Data-Type Comment Note dataset.data array(13233,2914) 每一行对应原始图片 62*47 像素 修改 slice_ 参数可能改变返回的 shape dataset.images array(13233,62,47) 每一行对应原始图片 62*47 像素 修改 slice_ 参数可能改变返回的 shape dataset.target array(13233) 每一行对应原始图片所属人物的ID - dataset.target_names array(13233) 人物名称 - from sklearn.datasets import fetch_lfw_people lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4) print(lfw_people.data[0:2]) print(lfw_people.target[0:2]) print(lfw_people.target_names) 3.2. lfw pairs sklearn.datasets.fetch_lfw_pairs 此数据集专用于比较 两张人脸 是否属于同一个人。 classes samples features 5749 13233 5828 Returns Data-Type Comment Note dataset.data array(2200,5828) 每一行对应两张原始图片 62*47 像素 shape 依赖于使用的子集 dataset.pairs array(2200,2,62,47) 每一行对应两张图片 - dataset.target array(2200) 两张图片是否对应一个人 - from sklearn.datasets import fetch_lfw_pairs lfw_pairs_train = fetch_lfw_pairs(subset='train') print(list(lfw_pairs_train.target_names)) print(lfw_pairs_train.data[0:2]) print(lfw_pairs_train.pairs[0:2]) print(lfw_pairs_train.target[0:2]) print(lfw_pairs_train.target_names[0:2]) 4. Forest covertypes sklearn.datasets.fetch_covtype 此数据集提供 581012 个植被（面积为30m*30m），可用以判断每个植被的覆盖类型（种植的植物）。 比较特殊的一点，特征值部分是 boolean 型，部分是 数值 型。 classes samples features 7 581012 54 Returns Data-Type Comment Note dataset.data array(581012,54) 每一行对应包含54个特征值 - dataset.target array(581012) 7种覆盖类型之一 - from sklearn.datasets import fetch_covtype covtype = fetch_covtype() print(covtype.data[0:2]) print(covtype.target[0:2]) print(covtype.target_names[0:2]) 5. RCV1 dataset sklearn.datasets.fetch_rcv1 路透社语料库，超过 800000 篇人工分类的报道 classes samples features 103 804414 47236 Returns Data-Type Comment Note dataset.data csr_array(804414,47236) 每一行对应包含47236个特征值 0.16%的非零值 dataset.target csr_array(804414,103) 103个分类标签，仅有0, 1值 3.15%的非零值 dataset.target_names array(103) 分类的名字 - dataset.sample_id array(804414) 报道ID - from sklearn.datasets import fetch_rcv1 rcv1 = fetch_rcv1() print(rcv1.data[0:2]) print(rcv1.target[0:2]) print(rcv1.target_names[0:2]) 6. Kddcup 99 dataset sklearn.datasets.fetch_kddcup99 此数据集是从一个模拟的美国空军局域网上采集来的9个星期的网络连接数据，分成具有标识的训练数据和未加标识的测试数据。 测试数据和训练数据有着不同的概率分布，测试数据包含了一些未出现在训练数据中的攻击类型，这使得入侵检测更具有现实性。 在训练数据集中包含了1种正常的标识类型normal和22种训练攻击类型。另外有14种攻击仅出现在测试数据集中。 训练数据集中每个连接记录包含了41个固定的特征属性和1个类标识：标识用来表示该条连接记录是正常的，或是某个具体的攻击类型。在41个固定的特征属性中，9个特征属性为离散(symbolic)型，其他均为连续(continuous)型。 classes samples features 23 4898431 41 7. California Housing dataset sklearn.datasets.fetch_california_housing 此数据集中包含房屋的8项属性，以预测房屋价格的中值。 samples features 20640 8 Returns Data-Type Comment Note dataset.data array(20640, 8) 每一行对应包含8个特征值 - dataset.target array(20640) 每一行对应平均楼房价格 - dataset.feature_names array(8) 特征值的名字 - from sklearn.datasets import fetch_california_housing california_housing = fetch_california_housing() print(california_housing.data[0:2]) print(california_housing.target[0:2]) print(california_housing.feature_names) "},"Content/Python/Scikit-Learn/Datasets/Generations.html":{"url":"Content/Python/Scikit-Learn/Datasets/Generations.html","title":"III * Generations","keywords":"","body":" Generations 1. Classification and Clustering 1.1. make-blobs 1.2. make-classification 1.3. make-gaussian-quantiles 1.4. make-hastie-10-2 1.5. make-circles 1.6. make-moons 1.7. make-multilabel-classification 1.8. make-biclusters 1.9. make-checkerboard 2. Regression 2.1. make-regression 2.2. make-sparse-uncorrelated 2.3. make_friedman1 2.4. make_friedman2 2.5. make_friedman3 3. Manifold 3.1. make-s-curve 3.2. make-swiss-roll 4. Decomposition 4.1. make-low-rank-matrix 4.2. make-sparse-coded-signal 4.3. make-spd-matrix 4.4. make-sparse-spd-matrix Generations 1. Classification and Clustering 分类 和 聚类 在数据集层面都是一样的，都是将整个数据集分离为多个数据集群，因此这里将两者归为一类。 1.1. make-blobs sklearn.datasets.make_blobs make_blobs 可通过制定 均值 和 标准差 来生成数据集。 Parameters Data-Type Default Comment Note n_samples int, array 100 如果为int，表示样本总数，会在各集群中平均分配；如果为array，表示各个集群的样本数量 - n_features int 2 每样本的特征数量 - centers int, array[n_centers,n_features] None 集群数量。如果n_samples为int，且centers为None，则集群数量为3。如果n_samples为array，则centers需为array[n_centers,n_features] 此参数可以认为是此函数最重要的参数，实际上它是指定了各集群的各特征的均值。如果为整数，则会通过随机数生成 cluster_std float, sequence of floats 1.0 集群的标准差 - center_box pair of floats (min,max) (-10.0,10.0) 随机生成的集群均值的边界范围 - from sklearn.datasets import make_blobs import matplotlib.pyplot as plt X, y = make_blobs(n_samples=100, centers=[[1, 1], [-10, 0]], random_state=0) plt.scatter(X[:,0], X[:,1], c=y) plt.show() 1.2. make-classification sklearn.datasets.make_classification make_classification 最大的特点是可以加入 噪音 影响。 make_classification 生成分类数据的方式是借助于超立方体（hypercube），步骤如下： 根据参数 n_informativen\\_informativen_informative，超立方体可具有的顶点最大数量为 2n_informative2^{n\\_informative}2​n_informative​​，这也是能生成的 clusterclustercluster 的最大数量； 根据参数 n_classes n\\_classes n_classes 和 n_clusters_per_class n\\_clusters\\_per\\_class n_clusters_per_class，n_classes×n_clusters_per_class n\\_classes \\times n\\_clusters\\_per\\_classn_classes×n_clusters_per_class 为真正生成的 clusterclustercluster 数量； 根据参数 class_sep class\\_sep class_sep，确定各个 clusterclustercluster 之间的距离，可以认为 2×class_sep 2 \\times class\\_sep 2×class_sep 是超立方体的边长； 根据参数 n_classes n\\_classes n_classes 和 n_clusters_per_class n\\_clusters\\_per\\_class n_clusters_per_class，把 clusterclustercluster 随机分散到各个 classclassclass，再通过正态分布随机生成样本点； Parameters Data-Type Default Comment Note n_samples int 100 样本的数量 - n_features int 20 总特征的数量，包含 n_informative、n_redundant、n_repeated，剩余部分为随机噪音 - n_informative int 2 有效特征的数量 - n_redundant int 2 多余特征的数量 由 有效特征 随机线性组合生成 n_repeated int 0 重复特征的数量 随机从 有效特征 和 多余特征 中选取 n_classes int 2 分类的数量 - n_clusters_per_class int 2 每分类中集群的数量 - weights list of floats, None None 每个分类占样本数量的比例 - flip_y float 0.01 随机交换各分类的样本的比例 目的是产生一定的噪音 class_sep float 1.0 各个 cluster 之间的距离，可认为超立方体的边长 较大的值会使得各 cluster 分隔的更远 hypercube boolean True 集群点是否放在超立方体的各个顶点 最好为True，各集群能分的更开 shift float, array[n_features], None 0.0 位移距离，feature 值的变化范围为 [shift-class_sep, shift+class_sep] - scale float, array[n_features], None 1.0 伸缩比例，feature值的变化范围为 [shift-class_sep*scale, shift+class_sep*scale] 伸缩在位移之后 Returns Data-Type Comment Note X array[n_samples,n_features] 生成的样本集 - y array[n_samples] 样本的分类标签 - from sklearn.datasets import make_classification import matplotlib.pyplot as plt X, y = make_classification(n_samples=200, n_features=2, n_informative = 2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, class_sep=5, random_state=0) plt.scatter(X[:,0], X[:,1], c=y) plt.show() 1.3. make-gaussian-quantiles sklearn.datasets.make_gaussian_quantiles make_gaussian_quantiles 是生成环形的分类，步骤如下： 以 mean 和 cov 作为正态分布的参数，对各个特征独立的随机生成特征值； 基于每个样本点与 mean 点的距离对样本点进行排序； 根据 n_samples 和 n_classes 确定每个分类的样本点数量，然后根据距离排序顺序分配； Parameters Data-Type Default Comment Note mean array[n_features] None 各特征的均值，如果为None，则均为0 - cov float 1.0 各个特征的方差 - n_samples int 100 总样本数量 - n_features int 2 特征数量 - n_classes int 3 分类的数量 - from sklearn.datasets import make_gaussian_quantiles import matplotlib.pyplot as plt X, y = make_gaussian_quantiles(n_samples=100, n_features=2, random_state=0) plt.scatter(X[:,0], X[:,1], c=y) plt.show() 1.4. make-hastie-10-2 sklearn.datasets.make_hastie_10_2 make_hastie_10_2 基于正态分布，独立生成10个特征的数据，并根据如下公式将其分为二类： y[i]={1if np.sum(X[i]∗∗2)>9.34−1if np.sum(X[i]∗∗2)⩽9.34 y[i] = \\begin{cases} 1 &\\text{if } np.sum(X[i] ** 2) > 9.34 \\\\ -1 &\\text{if } np.sum(X[i] ** 2) \\leqslant 9.34 \\end{cases} y[i]={​1​−1​​​if np.sum(X[i]∗∗2)>9.34​if np.sum(X[i]∗∗2)⩽9.34​​ from sklearn.datasets import make_hastie_10_2 import matplotlib.pyplot as plt X, y = make_hastie_10_2(n_samples=100, random_state=0) plt.scatter(X[:,0], X[:,1], c=y) plt.show() 1.5. make-circles sklearn.datasets.make_circles make_circles 生成内外两层圆圈，步骤如下： 根据参数 2π 2 \\pi2π 等分为 n_samples 份，得到每个样本的角度； 令 x=cos,y=sin x=\\cos,y=\\sin x=cos,y=sin，将角度转化为坐标，此为外圈样本的坐标； 将外圈坐标乘以 factor，得到内圈样本的坐标； 以 noise 为标准差形成零均值的正态分布，作为噪音加到原有X上； Parameters Data-Type Default Comment Note noise double None 噪音的标准差 - factor double[0,1] 0.8 内圈占外圈的比例因子 - from sklearn.datasets import make_circles import matplotlib.pyplot as plt X, y = make_circles(n_samples=100, noise=0.01, random_state=0) plt.scatter(X[:,0], X[:,1], c=y) plt.show() 1.6. make-moons sklearn.datasets.make_moons make_moons 生成两个交错的半圆，步骤基本与 make_circles 类似，只是将圆换成半圆，并且进行了一定的位移。 from sklearn.datasets import make_moons import matplotlib.pyplot as plt X, y = make_moons(n_samples=100, noise=0.01, random_state=0) plt.scatter(X[:,0], X[:,1], c=y) plt.show() 1.7. make-multilabel-classification 1.8. make-biclusters 1.9. make-checkerboard 2. Regression 2.1. make-regression sklearn.datasets.make_regression Parameters Data-Type Default Comment Note n_samples int 100 总样本数量 - n_features int 100 总特征数量 - n_informative int 10 有用的特征数量 - n_targets int 1 输出的回归值的维度数量，默认为1，即标量 - bias float 0.0 偏差项 - effective_rank int None - tail_strength float[0,1] 0.5 - noise float 0.0 噪音的标准差 - coef boolean False 是否返回系数 - Returns Data-Type Comment Note X array[n_samples,n_features] - y array[n_samples,n_targets] - coef array[n_features,n_targets] - 2.2. make-sparse-uncorrelated 2.3. make_friedman1 2.4. make_friedman2 2.5. make_friedman3 3. Manifold 3.1. make-s-curve 3.2. make-swiss-roll 4. Decomposition 4.1. make-low-rank-matrix https://zhidao.baidu.com/question/172378440.html 4.2. make-sparse-coded-signal 4.3. make-spd-matrix 4.4. make-sparse-spd-matrix "},"Content/Python/Scikit-Learn/Datasets/Other_Datasets.html":{"url":"Content/Python/Scikit-Learn/Datasets/Other_Datasets.html","title":"IV - Other Datasets","keywords":"","body":" Other Datasets 1. Sample images 2. svmlight or libsvm format 3. openml.org repository 4. external datasets Other Datasets 1. Sample images Scikit-learn 提供两张2D的JPEG图片：'china.jpg', 'flower.jpg'。 sklearn.datasets.load_sample_image sklearn.datasets.load_sample_images from sklearn.datasets import load_sample_image china = load_sample_image('china.jpg') print(china.dtype) print(china.shape) 需要注意，返回的数据是 uint8 格式，即无符号8字节int型，范围为0~255。如果有些场景应用需要转换为0~1，需要除以255。 2. svmlight or libsvm format svmlight/libsvm 格式的数据集，如下所示： : : ... 可通过如下方式加载： from sklearn.datasets import load_svmlight_file X_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\") X_train, y_train, X_test, y_test = load_svmlight_files( (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\")) 3. openml.org repository openml.org 是一个公共的机器学习的数据库，我们可以通过 sklearn.datasets.fetch_openml 从上面下载数据： from sklearn.datasets import fetch_openml mice = fetch_openml(name='miceprotein', version=4) print(mice.data.shape) print(mice.target.shape) print(np.unique(mice.target)) 4. external datasets scikit-learn 支持 numpy array、scipy sparse matrices、pandas DataFrame 格式的数据。 其他的数据，可通过： pandas.io 可读取 CSV, Excel, JSON, SQL 文件，转化为 scikit-learn 可使用的 DataFrame 格式； scipy.io 专用于 .mat、.arff 等二进制格式文件，此类数据常用于科学计算； numpy/routines.io 读取数据转化为 numpy arrays； 建议在存储数据时通过 HDF5 等格式已减少加载时间。 "},"Content/Python/Scikit-Learn/Preprocessing/":{"url":"Content/Python/Scikit-Learn/Preprocessing/","title":"5.1.2. Preprocessing","keywords":"","body":"3.1.1.2. Preprocessing "},"Content/Python/Scikit-Learn/Preprocessing/Feature_Extraction.html":{"url":"Content/Python/Scikit-Learn/Preprocessing/Feature_Extraction.html","title":"I * Feature Extraction","keywords":"","body":" Feature Extraction 1. Loading features from dicts 2. Feature hashing 3. Text feature extraction 4. Image feature extraction Feature Extraction Feature Extraction 主要是从 text 和 image 中提取特征。 1. Loading features from dicts Python 原生的 dict 数据结构，虽然不太快，但具有简单、稀疏、可读等特点，常用作数据存储。但是，在 scikit-learn 使用时，我们需要通过 类 DictVectorizer 将 dict 转化为 numpy.array 或 SciPy.CSR 格式。 DictVectorizer 将无序的离散分类特征（字符串）转化为 one-of-K（one-hot）编码，数值型特征保持不变。 Parameters Data-Type Default Comment Note dtype callable numpy.float64 转换后array或矩阵的元素类型 - separator string '=' one-hot 编码时使用的属性分隔符 - sparse boolean True 是否转化为 稀疏矩阵 - sort boolean True 转化时 特征名称 是否排序 - Attributes Data-Type Comment Note vocabulary_ dict 特征名称的字典，value为特征排序 - featurenames list 特征名称的数组 - Methods Parameters Returns Comment Note fit X（特征的字典） - 训练特征名称 - transform X（特征的字典） Xa（转化后的特征向量） 将特征转化为array或CSR - fit_transform X（特征的字典） Xa（转化后的特征向量） 训练并转化特征向量 - get_feature_names - 特征名称的数组 - - inverse_transform X（样本的特征矩阵） D（样本的特征映射） 将数组或稀疏矩阵X转换回特征映射 - restrict support - 对支持使用特征选择的模型进行特征限制，例如只选择前几个特征 - from sklearn.feature_extraction import DictVectorizer measurements = [{'city':'Beijing','country':'CN','temperature':33.},{'city':'London','country':'UK','temperature':12.},{'city':'San Fransisco','country':'USA','temperature':18.}] vec = DictVectorizer() measurements_vec = vec.fit_transform(measurements) print(vec.feature_names_) print(vec.vocabulary_) print(measurements_vec) print(measurements_vec.toarray()) 通过示例可以看出，多个离散分类特征（字符串）的 one-hot 编码的特征会加到一起，并不会做笛卡尔积。 2. Feature hashing Feature hashing 是降维的一种手段，将 高维度特征向量 转化为 低维度特征向量，主要通过类 FeatureHasher 实现。 Parameters Data-Type Default Comment Note n_features integer 1048576 输出矩阵的特征列数量 此值太小较容易引起特征碰撞 input_type string 'dict' 输入的数据类型 - dtype numpy type np.float64 特征值的类型，默认为浮点型 - alternate_sign boolean True 不同特征hash到同一特征时是否交替变化正负符号，以抵消hash碰撞的影响 - Methods Parameters Returns Comment Note transform raw_X（将被hash的特征集合） X（hash后的系数矩阵） 特征hash - from sklearn.feature_extraction import DictVectorizer from sklearn.feature_extraction import FeatureHasher measurements = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}] vec = DictVectorizer() measurements_vec = vec.fit_transform(measurements) print(measurements_vec) print(measurements_vec.toarray()) hash = FeatureHasher(n_features=10) measurements_hash = hash.transform(measurements) print(measurements_hash) print(measurements_hash.toarray()) 3. Text feature extraction 4. Image feature extraction "},"Content/Python/Scikit-Learn/Preprocessing/Preprocessing.html":{"url":"Content/Python/Scikit-Learn/Preprocessing/Preprocessing.html","title":"II * Preprocessing","keywords":"","body":" Preprocessing 1. StandardScaler 1.1. Parameters 1.2. Attributes 1.3. Methods 1.4. Examples Preprocessing sklearn.preprocessing OneHotEncoder 1. StandardScaler class sklearn.preprocessing.StandardScaler 标准化特征值 就是特征值减去均值（中心化，Centering）再除以标准差（缩放，Scaling）。 标准化特征值 对 每项特征 独立进行。 1.1. Parameters Parameters Data-Type Default Comment Note copy boolean True 是否返回原始数据缩放后的副本，不会修改原始数据 - with_mean boolean True 在缩放前是否中心化数据 稀疏数据尽量不要中心化，因为可能使得稀疏矩阵变成稠密矩阵 with_std boolean True 是否通过除以标准差的方式缩放 - 1.2. Attributes Attributes Data-Type Comment Note scale_ None, array[n_features] 各项特征的缩放比例 通过 np.sqrt(var_) 计算，如果 with_std=False 则返回 None mean_ None, array[n_features] 各项特征的均值 如果 with_mean=False 则返回 None var_ None, array[n_features] 各项特征的方差 如果 with_std=False 则返回 None n_samples_seen_ int, array[n_features] 各项特征计算使用的样本数量 如果特征值没有 NaNs，为 int，否则为 array 特征值为 NaNs 表示 missing，在 fit 时会忽略，transform 时会计算。 1.3. Methods 1.3.1. fit 计算 均值 和 标准差 供后续使用 Parameters Data-Type Default Comment Note X array[n_samples,n_features] - 训练集 - 1.3.2. fit_transform fit 后 transform Parameters Data-Type Default Comment Note X array[n_samples,n_features] - 原始训练集 - y array[n_samples] None 目标值 - Returns Data-Type Comment Note X_new array[n_samples,n_features_new] transform后新的训练集 - 1.3.3. get_params 获取参数集合 Parameters Data-Type Default Comment Note deep boolean True 如果True，会返回子对象 - Returns Data-Type Comment Note params mapping 参数名对应的map - 1.3.4. inverse_transform 逆 transform，即将 transform 后的训练集转换回 原始训练集 Parameters Data-Type Default Comment Note X array[n_samples,n_features] - 被 transform 后的训练集 - copy boolean None 是否拷贝输入的 X - Returns Data-Type Comment Note X_tr array[n_samples,n_features] 逆 transform 后的训练集 - 1.3.5. partial_fit 当样本集 数量特别大 或者 属于流式数据 时，我们可以通过 partial_fit 实现增量fit，也叫 online learning。 与 fit 相比，每一次的 partial_fit 都会结合之前的数据，而 fit 都是全新的计算。 Parameters Data-Type Default Comment Note X array[n_samples,n_features] - 原始训练集 - 1.3.6. set_params 设置参数，以便于人工调整 Parameters Data-Type Default Comment Note params mapping - 参数名对应的map - 1.3.7. transform 标准化处理：减去均值，除以标准差 Parameters Data-Type Default Comment Note X array[n_samples,n_features] - 原始训练集 - copy boolean None 是否拷贝输入的 X - 1.4. Examples from sklearn.preprocessing import StandardScaler data = [[0., 0.], [0., 0.], [1., 1.], [1., 1.]] scaler = StandardScaler() scaler.fit(data) print(\"scale_:\",scaler.scale_) print(\"mean_:\",scaler.mean_) print(\"var_:\",scaler.var_) print(\"n_samples_seen_:\",scaler.n_samples_seen_) print(scaler.transform(data)) print(scaler.transform([[2, 2]])) "},"Content/Python/Scikit-Learn/Dimensionality-Reduction/":{"url":"Content/Python/Scikit-Learn/Dimensionality-Reduction/","title":"5.1.3. Dimensionality Reduction","keywords":"","body":"3.1.1.3. Dimensionality Reduction "},"Content/Python/Scikit-Learn/Model-Selection/":{"url":"Content/Python/Scikit-Learn/Model-Selection/","title":"5.1.4. Model Selection","keywords":"","body":"3.1.1.4. Model Selection "},"Content/Python/Scikit-Learn/Model-Selection/Grid-Search/":{"url":"Content/Python/Scikit-Learn/Model-Selection/Grid-Search/","title":"5.1.4.1. Grid Search","keywords":"","body":"3.1.1.4.1. Grid Search "},"Content/Python/Scikit-Learn/Model-Selection/Cross-Validation/":{"url":"Content/Python/Scikit-Learn/Model-Selection/Cross-Validation/","title":"5.1.4.2. Cross Validation","keywords":"","body":"3.1.1.4.2. Cross Validation "},"Content/Python/Scikit-Learn/Model-Selection/Metrics/":{"url":"Content/Python/Scikit-Learn/Model-Selection/Metrics/","title":"5.1.4.3. Metrics","keywords":"","body":"3.1.1.4.3. Metrics "},"Content/Python/Scikit-Learn/Classification/":{"url":"Content/Python/Scikit-Learn/Classification/","title":"5.1.5. Classification","keywords":"","body":"3.1.1.5. Classification "},"Content/Python/Scikit-Learn/Regression/":{"url":"Content/Python/Scikit-Learn/Regression/","title":"5.1.6. Regression","keywords":"","body":"3.1.1.6. Regression "},"Content/Python/Scikit-Learn/Clustering/":{"url":"Content/Python/Scikit-Learn/Clustering/","title":"5.1.7. Clustering","keywords":"","body":"3.1.1.7. Clustering "},"Content/DataBase/":{"url":"Content/DataBase/","title":"6. DataBase","keywords":"","body":"3. DataBase "},"Content/DataBase/MySQL/":{"url":"Content/DataBase/MySQL/","title":"6.1. MySQL","keywords":"","body":"3.1 MySQL "},"Content/DataBase/OceanBase/":{"url":"Content/DataBase/OceanBase/","title":"6.2. OceanBase","keywords":"","body":"3.2 OceanBase "},"Content/Tools/":{"url":"Content/Tools/","title":"7. Tools","keywords":"","body":""},"Content/Tools/Git.html":{"url":"Content/Tools/Git.html","title":"I - Git","keywords":"","body":" Git 1. Glossary 1.1. Git 1.2. Repository 1.3. Working Tree 1.4. Index(Stage) 1.5. Head 1.6. Branch 2. Git 2.1. config 2.2. help 3. Repository 3.1. init 3.2. clone 4. Working Tree & Index(Stage) 4.1. add 4.2. status 4.3. diff 4.4. commit 4.5. rm 4.6. log 4.7. reset 4.8. checkout 5. Branching & Merging 5.1. branch 5.2 checkout 5.3. merge 6. Sharing and Updating Projects 6.1. pull 6.2. push 6.3. remote 7. FAQ 7.1. Set Chinese Log Git see 【Git Doc】 【Git教程】 1. Glossary 1.1. Git Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. 1.2. Repository Repository is a directory, which all the files under it will be controlled by Git. Usually, a repository means a project. 1.3. Working Tree Working tree is the local directory of repository, only this place could be directly edited by users. 1.4. Index(Stage) After \"add\" command, the change will be transfered from Working Tree to Index(Stage). Finally, after \"commit\" command, the change will be transfered from Index(Stage) to repository. 1.5. Head Every commit will generate a version. Head is a pointer to the newest version of current branch repository. 1.6. Branch Branch is the way to work on different versions of a repository at one time. By default your repository has one branch named master which is considered to be the definitive branch. We use branches to experiment and make edits before committing them to master.When you create a branch off the master branch, you’re making a copy, or snapshot, of master as it was at that point in time. 2. Git 2.1. config Get and set repository or global options. 2.2. help Display help information about Git. 3. Repository 3.1. init Create an empty Git repository or reinitialize an existing one. 3.2. clone Clone a repository into a new directory. 4. Working Tree & Index(Stage) 4.1. add Add file contents to the index. 4.2. status Show the working tree status. 4.3. diff Show changes between commits, commit and working tree, etc. 4.4. commit Record changes to the repository. 4.5. rm Remove files from the working tree and from the index. 4.6. log Show commit logs. 4.7. reset 1. Reset current version to specified version. It means the HEAD pointer change to specified version. 2. Reset files in Index(Stage), it means the change in Index(Stage) will turn back to working tree. 4.8. checkout Restore working tree files from Index(Stage) or repository. It means discard the changes in working tree. Tips: Command \"checkout\" can also be used to change branch, so in this scene, the comand must use with \"--\". 5. Branching & Merging 5.1. branch List, create, or delete branches. 5.2 checkout Switch branches. 5.3. merge Join two or more development histories together. 6. Sharing and Updating Projects 6.1. pull Fetch from and integrate with another repository or a local branch. 6.2. push Update remote refs along with associated objects. 6.3. remote Manage set of tracked repositories. 7. FAQ 7.1. Set Chinese Log git config --global core.quotepath false "},"Content/Tools/Homebrew.html":{"url":"Content/Tools/Homebrew.html","title":"II - Homebrew","keywords":"","body":" Homebrew 1. Command of Homebrew 1.1. brew --version 1.2. brew update 1.3. brew config 1.4. brew doctor 1.5. brew list 2. Command of formula 2.1. brew install 2.2. brew reinstall 2.3. brew uninstall 2.4. brew info Homebrew see Homebrew Documentation Homebrew is the easiest and most flexible way to install the UNIX tools Apple didn’t include with macOS. 1. Command of Homebrew 1.1. brew --version Check version of Homebrew. 1.2. brew update Fetch the newest version of Homebrew from GitHub. 1.3. brew config Show Homebrew and system configuration useful for debugging. If you file a bug report, you will likely be asked for this information if you do not provide it. 1.4. brew doctor Check your system for potential problems. Doctor exits with a non-zero status if any potential problems are found. Please note that these warnings are just used to help the Homebrew maintainers with debugging if you file an issue. If everything you use Homebrew for is working fine: please don’t worry or file an issue; just ignore this. 1.5. brew list List all installed formulae. 2. Command of formula 2.1. brew install Install formula. 2.2. brew reinstall Uninstall and then install formula (with existing install options). 2.3. brew uninstall Uninstall formula. 2.4. brew info Display information about formula. "}}
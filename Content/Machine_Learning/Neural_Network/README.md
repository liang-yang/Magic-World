<!-- toc -->

# Neural Network

Batch Normalization

[Batch Normalization原理与实战](https://zhuanlan.zhihu.com/p/34879333)

[Batch Normalization的通俗解释](https://zhuanlan.zhihu.com/p/54073204) 



- 梯度下降的动态学习步长；

- 梯度下降的限制条件；

- 怎么样初始化参数？


泛化方法：L1、L2范式，弃权，训练数据的人为扩展

更好的权重初始化方法

选择好的超参数


Dropout

[深度学习中Dropout原理解析](https://blog.csdn.net/program_developer/article/details/80737724)


neurons saturated

2006年，Hinton利用预训练方法缓解了局部最优解问题
 

一般现在的权重初始化是用Xavier。

数据集增益

<div align=center>![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8ym3oc7gxj30yb0u042c.jpg)




<!-- toc -->

# 分类模型的评估

---

本文主要分析 分类模型 的评估标准。  
实际上，搜索模型也可以采用同样的评估标准，只需把每一次搜索作为一个分类即可。

> 本文以 $$ N $$ 分类举例，样本集合为 $$ S $$，分类类别集合为 $$ C=\{c_i, c_2, ... c_{n-1}, c_n\} $$

## 1. 预测结果的分类

对于所有分类模型（不仅仅是二分类），其中每一个分类类别的预测结果均可以分为四类：

- **TP** —— True Positive
- **FP** —— Flase Positive
- **TN** —— True Negative
- **FN** —— False Negative

-|真实结果：$$c_i$$|真实结果：$$not \space c_i$$
:-:|:-:|:-:
**预测结果：$$c_i$$** | $$TP_{c_i}$$ | $$FP_{c_i}$$
**预测结果：$$not \space c_i$$** | $$FN_{c_i}$$ | $$TN_{c_i}$$

## 2. 分类的判别阈值

分类模型，主要包含两个部分：
- **分类算法**

    分类算法 基于样本的 业务指标 计算得到一个 分类指标（大多数算法是得到一个概率值）

- **判别阈值(Threshold)**

    分类指标 基于 判别阈值 对样本进行分类

也就是说，随着 判别阈值 的变化，样本分类也可能发生变化。同一分类算法，判别阈值 越高，$$TP,FP$$ 越少，$$FN,TN$$ 越多。

## 3. Accuracy

评估分类模型的预测结果，最直观的就是看预测的准确率，即 预测结果与真实结果相同的样本数量 在 总样本 中的占比，我们称之为 **Accuracy**【准确率】：

$$
Accuracy = \frac{\sum_{i = 1}^{n} TP_{c_i}}{|S|}
$$

一般来说，Accuracy 能直观的反映预测的情况。但是，我们很难量化的定义 Accuracy 的标准。也就是说，分类模型的 Accuracy 达到多少我们可以认为这个模型是优秀的？
举个医疗领域的例子。假设某病症的发病率为 0.01%，如果分类模型对所有输入样本均返回“未发病”（即发病的判别阈值>1），那么 Accuracy 可达到 99.99%。但是，这么“高”的 Accuracy 没有任何意义。

上面的案例中，Accuracy 失效的原因之一，是由于其是针对所有分类类别统一分析准确率，这样会掩藏部分分类类别的“不准确”，尤其是 **样本数量少但非常重要** 的分类类别。由此，我们认识到需要对各个分类类别分开分析，这就是 Precision、Recall 和 F-measure。

## 4. Precision & Recall & F-measure

### 4.1. Precision

Accuracy 是所有分类类别的预测结果的准确率，而 Precision【精确率/查准率】是指某分类类别的预测结果的准确率，例如 $$c_i$$ 的 Precision 为：

$$
Precision_{c_i} = \frac{TP_{c_i}}{TP_{c_i}+FP_{c_i}}
$$

一般来说，Precision 越高，此类别的分类效果越好。

但是，如果我们把 $$c_i$$ 的 分类判别阈值 定的很高，只在非常有把握的情况下才将样本判别为 $$c_i$$，这样可以明显提高 $$ Precision_{c_i} $$。但是，这样我们会遗漏大量的真实分类为 $$c_i$$ 的样本。因此，引入了 Recall。

### 4.2. Recall

Recall【召回率/查全率】，顾名思义，是指真实结果为某分类类别的所有样本被准确找到的比例。例如 $$c_i$$ 的 Recall 为：

$$
Recall_{c_i} = \frac{TP_{c_i}}{TP_{c_i}+FN_{c_i}}
$$

一般来说，Recall 越高，此类别的分类效果越好。

但是，如果我们把 $$c_i$$ 的 分类判别阈值 定的很低，甚至所有样本均判定为 $$c_i$$，这样 $$Recall_{c_i}$$ 可以达到很高。但实际上，这样并没有意义。

### 4.3. F-measure

理想情况下，我们希望 Precision 和 Recall 都很高。但实际场景中，两者一般是此消彼长的关系：随着判别阈值的提高，Precision 逐渐提高，Recall 逐渐降低。

因此，为了统一的评估分类模型，我们定义了 F-measure（也作F-score）：

$$
\frac{2}{F\_measure} = \frac{1}{Precision} + \frac{1}{Recall}
$$

$$
F\_measure = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}
$$

需要注意，F-measure 依然是针对某一分类类别而定义的。

### 4.4. Summary

回到上面提到的病症案例。如果分类模型对所有输入样本均返回“未发病”（即发病的判别阈值>1），那么：

分类类别|Precision|Recall|F-measure
:-:|:-:|:-:|:-:
**发病** | $$0\%$$ | $$0\%$$ | -
**未发病** | $$99\%$$ | $$100\%$$ | $$99.5\%$$

对比 Accuracy，Precision + Recall + F-measure 能细化到对每一个分类类别进行分析，我们就可以选择更关注的类别进行分析。例如，病症案例中，我们更关注 “发病”类别的指标。而从“发病”类别的指标来看，案例中的分类模型就非常不好，符合我们的判断。

## 5. PRC & AP & MAP

### 5.1. PRC(Precision Recall Curve)

Precision、Recall、F-measure 是 分类结果 的评估指标，而分类结果由 分类算法 和 判别阈值 共同决定。一般来说，分类算法相对比较固定，而 判别阈值 却可以有比较大的变化。因此，观察 判别阈值 的变化对各项评估指标的影响，可以帮助我们：

1. 选择某分类模型中更为合理的 判别阈值；
2. 更为全面的比较两个分类模型；

我们以 Precision 为纵轴，Recall 为横轴，将每个样本的值（如概率）作为判别阈值，多个样本形成多个PR点，进而形成 PRC 曲线。下图为两个分类模型的PRC曲线：

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fxfiiqfqy6j309u07i748.jpg)

> 需要注意，PRC 曲线的自变量是 判别阈值，更准确的说是样本的分类指标。

我们分析PRC曲线的四个边界点：

1. **(0,0)**：Recall 为 0，Precision 为 0，说明此分类模型特别不好，一个正确的分类都没有找到；
2. **(0,1)**：Recall 为 0，Precision 为 1，说明此分类模型的 判定阈值 特别高，几乎全部判定为负样本，这样即使准确率高也不好；
3. **(1,0)**：Recall 为 1，Precision 为 0，说明此分类模型的 判定阈值 特别低，几乎将所有样本都归为正样本，这样即使查全率高也不好；
4. **(1,1)**：Recall 为 1，Precision 为 1，说明此分类模型非常好，分类又全又准；

因此：

1. 单条PRC曲线中，越靠近右上角(1,1)的点的 判别阈值 对应的 评估指标 综合性更好（对于某些场景，更在意 Precision 和 Recall 中的一个，就不需要靠近右上角）；
2. 多条PRC曲线中，越在上的曲线对应的 分类模型 更好，如上图中红色曲线更好。这是因为上方的曲线对同样的 Precision 有更高的 Recall，对同样的 Recall 有更高的 Precision；

另外，相对光滑的曲线更为稳定，不会因 判别阈值 的轻微变化导致 评估指标 剧烈抖动。

### 5.2. AP(Average Precision)

观察 PRC 曲线较为抽象，既然认为上方的曲线更好，那么很容易想到以 PRC 曲线下的面积作为指标进行判定，这就是 AP(Average Precision)，这里的 Average 相当于是对 Precision 取均值。

### 5.3. MAP(Mean Average Precision)

需要注意，Precision、Recall、F-measure、PRC、AP 都是针对单个分类类别的，如果我们需要分析整体分类的情况，就需要对所有类别求均值，即
$$
MAP_{c_i} = \frac{1}{|S|} \cdot \sum_{i=1}^{n} AP_{c_i}
$$

> MAP仅在所有分类类别都同等重要的情况下使用，例如用作搜索模型的评估，每一个搜索关键字都可以作为一个分类类别，再通过MAP汇聚结果。而之前医药案例中我们有侧重的分类类别，就可以不关注MAP，否则可能导致误导。

## 6. ROC & AUC

PRC 曲线相关的评估指标，是分类模型的一套评估体系。另外，还有 ROC 曲线相关的一套评估体系。我们先给出定义，然后比较两者的使用场景。

> see [ROC & AUC](http://alexkong.net/2013/06/introduction-to-auc-and-roc/)

### 6.1. TPR & FPR

- $$TPR$$ : True Positive Rate，所有正样本中被预测为正的比例
，也就是 Recall
$$
TPR_{c_i} = \frac{TP_{c_i}}{TP_{c_i} + FN_{c_i}} = Recall_{c_i}
$$
- $$FPR$$ : False Positive Rate，所有负样本中被预测为正的比例
$$
FPR_{c_i} = \frac{FP_{c_i}}{TN_{c_i} + FP_{c_i}}
$$

### 6.2. ROC(Receiver Operating Characteristic)

ROC 曲线以 TPR 为纵轴，FPR 为横轴，将每个样本的值（如概率）作为判别阈值，多个样本形成 ROC 曲线。下图为三个分类模型的 ROC 曲线：

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fxgn3uwv55j30dw0dhglw.jpg)

> 与 PRC 曲线一样，ROC 曲线的自变量也是 判别阈值

我们分析 ROC 曲线的四个边界点：

1. **(0,0)**：FPR 为 0，TPR 为 0，说明此分类模型的 判定阈值 特别高，全部判定为负样本；
2. **(0,1)**：FPR 为 0，TPR 为 1，说明此分类模型非常好，分类又全又准；
3. **(1,0)**：FPR 为 1，TPR 为 0，说明此分类模型特别不好，一个正确的分类都没有找到；
4. **(1,1)**：FPR 为 1，TPR 为 1，说明此分类模型的 判定阈值 特别低，全部判定为正样本；

因此：

1. 单条 ROC 曲线中，越靠近左上角(0,1)的点的 判别阈值 对应的 评估指标 综合性更好（对于某些场景，更在意 FPR 和 TPR 中的一个，就不需要靠近左上角）；
2. 多条 ROC 曲线中，越在上的曲线对应的 分类模型 更好。这是因为上方的曲线对同样的 TPR 有更小的 FPR，对同样的 FPR 有更高的 TPR；

### 6.3. AUC(Area Under ROC Curve)

和 AP 类似，AUC 是指 ROC 曲线下的面积，显然这个面积的数值不会大于1。使用AUC 作为评价标准是因为很多时候 ROC 曲线并不能清晰的说明哪个分类模型的效果更好，而作为一个数值，AUC 更大的分类模型效果更好。

## 7. ROC vs PRC

> see [ROC vs PRC](https://blog.csdn.net/pipisorry/article/details/51788927)

### 7.1. ROC 与 PRC 的评估结果一致

同一数据集，不同分类算法的 ROC 和 PRC 的优劣一致：如果A算法的 ROC 优于B算法，那么A算法的 PRC 也一定优于B算法，反之亦然。
    
> see 《The Relationship Between Precision-Recall and ROC Curves》

那么，针对不同分类模型间进行比较时，逻辑上 ROC 和 PRC 的结果是一致的。

### 7.2. PRC右上角最优 vs TOC左上角最优

1. PRC 横轴是 Recall，纵轴是 Precision，这两个指标都是越大越好，因此右上角最优；
2. TOC 横轴是 FPR，越小越好；纵轴是 TPR，也就是 Recall，越大越好。因此左上角最优；
    
### 7.3. ROC 比 PRC 更平滑

首先，我们看看同样的分类模型在不同的样本集中 ROC 和 PRC 的差异。下图中：  
1. (a)和(c)为 ROC 曲线，(b)和(d)为 PRC 曲线；   
2. (a)和(b)的样本集中正负样本数量比例为1：1；   
3. (c)和(d)的样本集中正负样本数量比例为1：10；    
    
![](https://ws1.sinaimg.cn/large/006tNbRwgy1fxgwa3ni9cj30hc0gwq3g.jpg)
    
可以看出，在不同的样本集中：  
1. ROC 始终很平滑；   
2. PRC 有时很平滑，有时剧烈变化。  
    
这是由于两个曲线不同的组成指标的公式决定的：   
1. TPR/Recall 的分母为“真实正样本的数量”，一旦样本集确定，就不再因 判别阈值 的变化而变化。分子为“预测准确的正样本的数量”，随着 判别阈值 的变化单调变化，比较平滑；     
2. FDR 的分母为“真实负样本的数量”，一旦样本集确定，就不再因 判别阈值 的变化而变化。分子为“预测错误的负样本的数量”，随着 判别阈值 的变化单调变化，比较平滑；
3. Precision 的分母为“预测为正样本的数量”，分子为“预测准确的正样本的数量”，都会随着判别阈值 的变化而变化，因此 Precision 的变化整体不单调，变化比较剧烈。

因此，我们可以得出结论：在大部分样本集上，**ROC 比 PRC 更平滑**，这也是 ROC 更常用的原因之一。
    
但是，我们需要清楚的认识到，ROC 比 PRC 更平滑只是其变化趋势较缓，并不代表 TOC 比 PRC 更准确。
    
### 7.4. ROC 不一定比 PRC 更准确

下图为 同一样本集 的 ROC 和 PRC：
1. ROC 很靠近（0,1）点，说明分类模型很好；
2. PRC 离（1,1）点比较远，说明分类模型不太好；
两个曲线得到不同的结论，那么哪个更准确呢？

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fxhtqt8s2ij30go085jrm.jpg)

两个曲线中的 红点，其实是同一个点，结合两张图我们可以得到：

$$
TPR = Recall = 0.8 ,\quad Precision = 0.05, \quad FPR = 0.1
$$

我们假设 真实正样本数量为 100，即 TP + FN = 100：
1. 结合 TPR = 0.8，得到 TP = 80，FN =  20；
2. 结合 Precision = 0.05，得到 FP = 1520；
3. 结合 FPR = 0.1，得到 TN = 13680；

通俗的说，有 100 个正样本，15200 个负样本，这种情况下我们更看重将 正样本 分类正确的表现。而曲线对应的分类模型在 红点 处将 1520 + 80 = 1600 个样本判定为正样本，但其中只有 80 个正确。主观上还是认为这个分类模型有提升空间的。

那么，为什么 ROC 和 PRC 会有这个差异呢？这是由于 PRC 的 Precision、Recall 都是针对我们在意的分类类别在分析，例如上例就是仅分析 正样本 的分类表现，不在意 负样本 的分类表现。而 ROC 的 TPR、FPR 综合考虑了两种分类类别的分析。

因此，我们可以得出结论：**ROC 不一定比 PRC 更准确**。所以，我们最好两条曲线都进行观察，然后针对不同的场景进行选择。

## 8. Demo of Python

我们可以通过 sklearn 包计算：   
ROC —— [sklearn.metrics.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)    
AUC —— [sklearn.metrics.auc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html)   
PRC —— [sklearn.metrics.precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)    
AP —— [sklearn.metrics.average_precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)

参考官网的案例，我们可以画出 ROC 并计算 AUC

{%ace edit=true, lang='python'%}

import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt

y = np.array([1, 1, 2, 2])
scores = np.array([0.1, 0.4, 0.35, 0.8])
fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)
auc = metrics.auc(fpr, tpr)

print("fpr:",fpr)
print("tpr:",tpr)
print("thresholds:",thresholds)
print(auc)

plt.plot(fpr,tpr,marker = 'o')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.show()

{%endace%}

执行结果如下：

> fpr: [0.  0.5 0.5 1. ]   
tpr: [0.5 0.5 1.  1. ]   
thresholds: [0.8  0.4  0.35 0.1 ]   
auc: 0.75   

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fxi50wjyxhj30fy0co3yf.jpg)

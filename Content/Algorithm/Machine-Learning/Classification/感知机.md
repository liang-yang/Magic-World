<!-- toc -->

# 感知机(Perceptron)

---

感知机是二分类的线性分类模型，在特征空间中通过一个超平面将特征向量分为两类，是神经网络和支持向量机的基础。下面按照机器学习三要素（模型集合、目标策略、学习算法）对感知机进行分析。

> see [Perceptron](http://www.hankcs.com/ml/the-perceptron.html)

## 1.1. 模型集合

感知机是一种线性分类模型。对于分类模型，假设输入空间（特征空间）是 $$ x \in R^n $$，输出空间为 $$ y \in \{+1,-1\} $$，那么分类模型统一的公式为：

$$
y = Classification(x)
$$

感知机模型，是通过超平面（线性模型）将所有特征向量进行分隔，在超平面两侧的特征向量分别为两类。我们知道，超平面 的表达式为：

$$
w \cdot x + b = 0
$$

其中，$$ w \cdot x $$ 表示内积，$$ w \in R^n $$ 称作权值向量，$$ b \in R $$ 称作偏置，均为模型参数。

在超平面两侧分别为两类，则分别为 $$ w \cdot x + b > 0 $$ 和 $$ w \cdot x + b < 0 $$，那么可以使用符号函数 $$ sign $$ 作为分类函数：

$$
sign(t) = \begin{cases}
    +1 &\text{if } t	\geqslant 0  \\
    -1 &\text{if } t  < 0
	\end{cases}
$$

由此得到 感知机分类模型 的完整公式：

$$
y = sign(w \cdot x + b)
$$

感知机的模型集合为特征空间中所有的线性分类模型，也可以理解为对应维度的所有超平面，我们根据 目标策略 和 学习算法 找出最合适的超平面即可。

## 1.2. 目标策略

按照感知机的分类目标，最直观的策略是误分类点的个数：误分类点越少感知机模型越好。但是，误分类点的个数不连续，更不可导，而损失函数我们一般希望其连续可导以便于计算。因此，替代将 **所有误分类点到超平面的总距离** 作为损失函数。

假设超平面为 $$ w \cdot x +b = 0 $$，那么点 $$ x_0 $$ 与此超平面的绝对距离为：

$$
L(w,b)_{x_0} = \frac{|w \cdot x_0 +b|}{||w||}
$$

其中 $$ ||w|| $$ 表示 $$ w $$ 的 $$ L_2 $$ 范数。在感知机中，我们仅需要判断正负及比较相对距离，因此可不除以 $$ ||w|| $$，取相对距离即可。

对于训练集 $$ T=\{(x_1,y_1),(x_2,y_2),...(x_k,y_k)\} $$，其中 $$ x_i \in R^n $$，$$ y_i \in \{+1,-1\} $$。假设 $$ M $$ 为超平面误分类点的集合，则损失函数为：

$$
L(w,b)=-\sum_{x_i \in M} y_i(w \cdot x_i + b)
$$

感知机的 目标策略 即求使得此损失函数最小的模型参数 $$ w,b $$ 。

## 1.3 学习算法

### 1.3.1. 梯度下降法

按照常规的方法，求极值就是对损失函数的未知参数求导为0。但深入的分析，感知机的损失函数有一个前提条件：误分类点的集合固定。而实际随着参数的变化，误分类点会发生变化，也就是说误分类点集合不固定。因此，我们不能通过求导的方式来求极值。

所以，我们采用 梯度下降法，

- **计算方法**

    感知机的学习算法是采取 **随机梯度下降法**，即不断的随机选择误分类点，使得选中的误分类点的损失函数逐渐变小，一直到正确分类为止。原因有二：
    
    1. 误分类点集合不固定。 也就是说，我们不能按照常规的方法来求解极值。
    
    2. 同样是梯度下降法，为什么选择 ++随机梯度下降法++ 而不选择 ++批量梯度下降法++ ？这主要是基于计算量的考虑。批量梯度下降法 每一轮迭代都需要所有实例进入计算，而随机梯度下降法是每次仅少量实例甚至单个实例进入计算。当然，批量梯度下降是肯定得到全局最优解，而随机梯度下降在多峰值情况下有可能得到局部最优解。—— 参考 [随机梯度下降和批量梯度下降的公式对比](http://blog.csdn.net/lilyth_lilyth/article/details/8973972)  
    梯度的计算公式如下：
   $$
    \nabla_w L(w,b)=-\sum_{x_i \in M} y_i x_i
    
    \nabla_b L(w,b)=-\sum_{x_i \in M} y_i
   $$
    
    使用随机梯度下降法，如果训练集能完全线性分类，则此过程为收敛过程，即可以在有限次梯度下降后实现所有训练集线性分类。（收敛的证明没太看明白，但有一些几何上的理解，后面进行说明）

- **原始形式**
    
    【输入】：训练集 $$ T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\} $$，其中 $$ x_i \in R^n $$，$$ y_i \in \{+1,-1\} $$，学习率 $$ \eta(1 \geqslant \eta > 0) $$：

    【输出】：$$ w, b $$；感知机模型 $$ f(x)=sign(w \cdot x+b) $$
    
    1. 随机选取初始值 $$ w_0,b_0 $$；
    2. 在训练集中选取数据 $$ (x_i,y_i) $$；
    3. 如果 $$ y_i(w \cdot x_i+b) \leqslant 0 $$:
   $$
    w = w + \eta y_i x_i
    
    b = b + \eta y_i
   $$
    4. 转至步骤2，直至训练集中没有误分类点。

- **对偶形式**

    由于参数 $$ w,b $$ 每一轮迭代都会发生改变，导致每一轮的内积计算 $$ w \cdot x_i $$ 都需要重新开始计算，效率较低，所以考虑通过其他形式提升计算性能。
    
    从原始形式的计算方式可以看出：
   $$
    w = \sum_{i=1}^N n_i \eta y_ix_i = \sum_{i=1}^N \alpha_i y_ix_i
    
    b = \sum_{i=1}^N n_i \eta y_i = \sum_{i=1}^N \alpha_i y_i
   $$
    其中，$$ n_i $$ 表示某误分类点的修改次数，$$ \alpha_i = n_i \eta $$ 作为参数。由此，得到原始形式的对偶形式。
    
    【输入】：训练集 $$ T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\} $$，其中 $$ x_i \in R^n $$，$$ y_i \in \{+1,-1\} $$，学习率 $$ \eta(1 \geqslant \eta > 0) $$;
    
    【输出】：$$ \alpha, b $$；感知机模型 $$ f(x)=sign(\sum_{i=1}^N \alpha_i y_ix_i \cdot x+b) $$，其中 $$ \alpha = (\alpha_1,\alpha_2...,\alpha_N)^T $$。
    
    1. 选取初始值 $$ \alpha = (0,0,...,0), b = 0 $$；
    2. 在训练集中选取数据 $$ (x_i,y_i) $$；
    3. 如果 $$ y_i(\sum_{i=j}^N \alpha_j y_jx_j \cdot x_i+b) \leqslant 0 $$:
   $$
    \alpha_i = \alpha_i + \eta
    
    b = b + \eta y_i
   $$
    4. 转至步骤2，直至训练集中没有误分类点。
    
    可以看出，对偶形式中内积运算是固定的，我们可以预先把训练集中实例间的两两内积计算出来并以矩阵的形式存储，这个矩阵就是 Gram 矩阵。
   $$
    G = [x_i \cdot x_j]_{N \times N}
   $$
    基于 Gram 矩阵运算，可以提升计算效率。
    
    —— 参考 [理解感知机学习算法的对偶形式](https://www.zhihu.com/question/26526858)
    
## 1.4 几何意义

我们以二维平面为例对几何意义进行说明。

训练集中的实例相当于平面中的点，感知机算法就相当于求一条直线，将正负两类分隔开。我们不妨假设此直线方程为：
$$
w \cdot x + b = 0
$$
其中，$$ w $$ 是直线的法向量，$$ b $$ 是原点到直线的截距。（严格的说，应该称 $$ w/\|w\| $$ 为单位法向量，$$ b/\|w\| $$ 为原点到直线的物理截距）

因此，在算法中 $$ w = w + \eta y_i x_i $$ 就可以理解为将直线的法向量往误分类点 $$ x_i $$ 倾斜， $$ b = b + \eta y_i $$ 就可以理解为将直线沿法向量往误分类点 $$ x_i $$ 移动。简单的说，就是一个调整方向，一个调整距离，最终使得直线越过该误分类点使其被正确分类。

感知机收敛性的证明，几何意义上，即我们认为每一次的移动，都考虑了误分类点的因素，循环迭代，就是将多个因素进行融合，最终得到一条可完整分隔的超平面。

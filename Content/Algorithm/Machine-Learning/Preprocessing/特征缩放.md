<!-- toc -->

# 特征缩放(Feature Scaling)

---

> see [Feature scaling](https://en.wikipedia.org/wiki/Feature_scaling)

特征缩放 是在数据预处理阶段进行的数据标准化的方法。

这是由于，原始数据的值范围往往变化很大，在一些算法中，如果没有标准化，可能效果会很差，例如：

1. 我们经常使用 欧式距离 计算两点之间的距离。但如果点向量中某项要素变化范围很大，那么距离就会严重依赖此项要素的值；
2. 梯度下降方法中，特征缩放会使得算法收敛效率快的多；

特征缩放 根据 目的不同，大概分为 归一化、中心化、标准化 三类。

## 1. 归一化(Normalization)

归一化 的目的，是将数据的范围限定在一个较小的范围（如 $$[0,1]$$），并且消除量纲的影响。

假设样本集 $$ S = \{\vec{x}_1,\vec{x}_2,...,\vec{x}_N\} $$，其中样本 $$ \vec{x}_i = (x_{i,1},x_{i,2},...,x_{i,k}) $$ 是表示 $$ k $$ 个特征项的 $$ k $$ 维向量。

那么，根据视角的不同，可以将 归一化 分为两种：

### 1.1. 不同样本同一特征

基于样本集的某项特征进行归一化，而不考虑各个特征之间的关系：

- **Rescaling (min-max normalization)**

$$
x_{i,j}' = \frac{x_{i,j} - \min(x_{*,j})}{\max(x_{*,j}) - \min(x_{*,j})}
$$

- **Mean normalization**

$$
x_{i,j}' = \frac{x_{i,j} - mean(x_{*,j})}{\max(x_{*,j}) - \min(x_{*,j})}
$$

此类型方法，当有新数据加入时，可能使得 $$ mean $$、$$ \max $$、$$ \min $$ 发生变化，进而导致所有的样本数据均需要重新计算。

### 1.2. 同一样本不同特征

此种方法在单个样本的各个特征之间进行归一化，而不考虑各个样本之间的关系：

- **Scaling to unit length**

$$
x_{i,j}' = \frac{x_{i,j}}{||\vec{x}_i||} = \frac{x_{i,j}}{\sqrt{x_{i,1}^2 + x_{i,2}^2 + ... + x_{i,k}^2}}
$$

此方法可使得所有样本的特征向量的长度均固定为1，更便于样本间进行分析。

## 2. 中心化(Centered)

中心化 的目的，是将样本集中某项特征值的均值转化为零，这样，这些特征值就会分布在零点周围。

$$
x_{i,j}' = x_{i,j} - mean(x_{*,j}) 
$$

> 中心化 依然是基于样本集的某项特征进行缩放，而不考虑各个特征之间的关系。

## 3. 标准化(Standardization)

一般来说，我们都假设特征值的分布是正态分布。因此，标准化 的目的是将样本集中某项特征值的正态分布转化为 标准正态分布，即 均值为0、方差为1的正态分布。

$$
x_{i,j}' = \frac{x_{i,j} - mean(x_{*,j})}{\sigma_j} 
$$

> 标准化 依然是基于样本集的某项特征进行缩放，而不考虑各个特征之间的关系。

## 4. 归一化 vs 标准化

- **同**：

    1. 归一化 和 标准化 均会将特征值缩放；
    2. 归一化 和 标准化 均不会影响单项特征值的分布；
    
- **异**：    
    
    1. 归一化 将特征值缩放到 $$ [0,1] $$ 的范围内，而 标准化 将特征值缩放到 $$ 0 $$ 周围；
    2. 归一化 会将所有特征值均缩放到 $$ [0,1] $$ ，虽不影响单项特征值的分布，但会影响各项特征值之间的比较。 而 标准化 除保留单项特征值的分布外，也会保留各项特征值之间的比较；
    3. 例如下例中，由于量纲的影响，原始数据表征为椭圆形，而经过归一化处理后，会变化为圆形。而 标准化 处理后图形依然会表征为椭圆形；

    ![](https://ws2.sinaimg.cn/large/006tNbRwgy1fxzrmrrvaoj30k0038dfw.jpg)

    ![](https://ws1.sinaimg.cn/large/006tNbRwgy1fxzrkq167dj30jp09z3yq.jpg)


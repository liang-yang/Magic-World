<!-- toc -->

# 分类模型的评估

本文主要分析 分类模型 的评估标准。  
实际上，搜索模型也可以采用同样的评估标准，只需把每一次搜索作为一个分类即可。

> 本文以 $$ N $$ 分类举例，样本集合为 $$ S $$，分类类别集合为 $$ C=\{c_i, c_2, ... c_{n-1}, c_n\} $$

## 1. 预测结果的分类

对于所有分类模型（不仅仅是二分类），其中每一个分类类别的预测结果均可以分为四类：

- **TP** —— True Positive
- **FP** —— Flase Positive
- **TN** —— True Negative
- **FN** —— False Negative

-|真实结果：$$c_i$$|真实结果：$$not \space c_i$$
:-:|:-:|:-:
**预测结果：$$c_i$$** | $$TP_{c_i}$$ | $$FP_{c_i}$$
**预测结果：$$not \space c_i$$** | $$FN_{c_i}$$ | $$TN_{c_i}$$

## 2. 分类的判别阈值

分类模型，主要分别两个部分：
- **分类算法**

    分类算法 基于样本的业务指标计算得到一个分类指标（大多数算法是得到一个概率值）.

- **判别阈值**

    分类指标 基于 判别阈值 对样本进行分类.

也就是说，随着 判别阈值 的变化，样本分类也可能发生变化。同一分类算法，判别阈值 越高，$$TP,FP$$ 越少，$$FN,TN$$ 越多。

## 3. Accuracy

评估分类模型的预测结果，最直观的就是看预测的准确率，即 预测结果与真实结果相同的样本数量 在 总样本 中的占比，我们称之为 **Accuracy**【准确率】：

$$
Accuracy = \frac{\sum_{i = 1}^{n} TP_{c_i}}{|S|}
$$

一般来说，Accuracy 能直观的反映预测的情况。但是，我们很难量化的定义 Accuracy 的标准。也就是说，分类模型的 Accuracy 达到多少我们可以认为这个模型是优秀的？
举个医疗领域的例子。假设某病症的发病率为 0.01%，如果分类模型对所有输入样本均返回“未发病”（即发病的判别阈值>1），那么 Accuracy 可达到 99.99%。但是，这么“高”的 Accuracy 没有任何意义。

上面的案例中，Accuracy 失效的原因之一，是由于其是针对所有分类类别统一分析准确率，这样会掩藏部分分类类别的“不准确”，尤其是 **样本数量少但非常重要** 的分类类别。由此，我们认识到需要对各个分类类别分开分析，这就是 Precision、Recall 和 F-measure。

## 4. Precision & Recall & F-measure

### 4.1. Precision

Accuracy 是所有分类类别的预测结果的准确率，而 Precision【精确率/查准率】是指某分类类别的预测结果的准确率，例如 $$c_i$$ 的 Precision 为：

$$
Precision_{c_i} = \frac{TP_{c_i}}{TP_{c_i}+FP_{c_i}}
$$

一般来说，Precision 越高，此类别的分类效果越好。

但是，如果我们把 $$c_i$$ 的 分类判别阈值 定的很高，只在非常有把握的情况下才将样本判别为 $$c_i$$，这样可以明显提高 $$ Precision_{c_i} $$。但是，这样我们会遗漏大量的真实分类为 $$c_i$$ 的样本。因此，引入了 Recall。

### 4.2. Recall

Recall【召回率/查全率】，顾名思义，是指真实结果为某分类类别的所有样本被准确找到的比例。例如 $$c_i$$ 的 Recall 为：

$$
Recall_{c_i} = \frac{TP_{c_i}}{TP_{c_i}+FN_{c_i}}
$$

一般来说，Recall 越高，此类别的分类效果越好。

但是，如果我们把 $$c_i$$ 的 分类判别阈值 定的很低，甚至所有样本均判定为 $$c_i$$，这样 $$Recall_{c_i}$$ 可以达到很高。但实际上，这样并没有意义。

### 4.3. F-measure

理想情况下，我们希望 Precision 和 Recall 都很高。但实际场景中，两者一般是此消彼长的关系：随着判别阈值的提高，Precision 逐渐提高，Recall 逐渐降低。

因此，为了统一的评估分类模型，我们定义了 F-measure（也作F-score）：

$$
\frac{2}{F\_measure} = \frac{1}{Precision} + \frac{1}{Recall}
$$

$$
F\_measure = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}
$$

需要注意，F-measure 依然是针对某一分类类别而定义的。

### 4.4. Summary

回到上面提到的病症案例。如果分类模型对所有输入样本均返回“未发病”（即发病的判别阈值>1），那么：

分类类别|Precision|Recall|F-measure
:-:|:-:|:-:|:-:
**发病** | $$0\%$$ | $$0\%$$ | -
**未发病** | $$99\%$$ | $$100\%$$ | $$99.5\%$$

对比 Accuracy，Precision + Recall + F-measure 能细化到对每一个分类类别进行分析，我们就可以选择更关注的类别进行分析。例如，病症案例中，我们更关注 “发病”类别的指标。而从“发病”类别的指标来看，案例中的分类模型就非常不好，符合我们的判断。

## 5. PRC & AP & MAP

### 5.1. PRC(Precision Recall Curve)

Precision、Recall、F-measure 是 分类结果 的评估指标，而分类结果由 分类算法 和 判别阈值 共同决定。一般来说，分类算法相对比较固定，而 判别阈值 却可以有比较大的变化。因此，我们来观察 判别阈值 的变化对各项评估指标的影响：

1. 选择某分类模型中更为合理的 判别阈值；
2. 更为全面的比较两个分类模型；

我们以 Precision 为纵轴，Recall 为横轴，将每个样本的值（如概率）作为判别阈值，多个样本形成多个PR点，进而形成 PRC 曲线。下图为两个分类模型的PRC曲线：

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fxfiiqfqy6j309u07i748.jpg)

> 需要注意，PRC 曲线的自变量是 判别阈值，更准确的说是样本的分类指标。

我们分析PRC曲线的四个边界点：

1. **(0,0)**：Recall 为 0，Precision 为 0，说明此分类模型特别不好，一个正确的分类都没有找到；
2. **(0,1)**：Recall 为 0，Precision 为 1，说明此分类模型分类很准确，但
3. **(1,0)**：Recall 为 0，Precision 为 0，说明
4. **(1,1)**：Recall 为 0，Precision 为 0，说明此分类模型非常好，所有分类都很正确；

因此：

1. 单条PRC曲线中，越靠近右上角(1,1)的点的 判别阈值 对应的 评估指标 更好；
2. 多条PRC曲线中，越在上的曲线对应的 分类模型 更好，如上图中红色曲线更好。这是因为上方的曲线对同样的 Precision 有更高的 Recall，对同样的 Recall 有更高的 Precision；

另外，相对光滑的曲线更为稳定，不会因 判别阈值 的轻微变化导致 评估指标 剧烈变化。

### 5.2. AP(Average Precision)

观察 PRC 曲线较为抽象，既然认为上方的曲线更好，那么很容易想到以 PRC 曲线下的面积作为指标进行判定，这就是 AP(Average Precision)，这里的 Average 相当于是对 Precision 取均值。

### 5.3. MAP(Mean Average Precision)

需要注意，Precision、Recall、F-measure、PRC、AP 都是针对单个分类类别的，如果我们需要分析整体分类的情况，就需要对所有类别求均值，即
$$
MAP_{c_i} = \frac{1}{|S|} \cdot \sum_{i=1}^{n} AP_{c_i}
$$

> MAP仅在所有分类类别都同等重要的情况下使用，例如用作搜索模型的评估，每一次的搜索都可以作为一次分类，再通过MAP汇聚结果。而之前医药案例中我们有侧重的分类类别，就可以不关注MAP，否则还可能导致误导。

## 6. ROC & AUC

### 6.1. TPR & FPR

- $$TPR$$ : True Positive Rate，所有正样本中被预测为正的比例
，也就是 Recall
$$
TPR = \frac{TP}{TP + FN}
$$
- $$FPR$$ : False Positive Rate，所有负样本中被预测为正的比例
$$
FPR = \frac{FP}{TN + FP}
$$



### 6.2. ROC(Receiver Operating Characteristic)

ROC曲线的横坐标为 FPR，纵坐标为 TPR


### 6.3. AUC(Area Under ROC Curve)




https://blog.csdn.net/pipisorry/article/details/51788927

http://alexkong.net/2013/06/introduction-to-auc-and-roc/

https://blog.csdn.net/jningwei/article/details/78955536

https://www.cnblogs.com/sddai/p/5696870.html


<!-- toc -->

# 分类模型的评估

> 本文以四分类举例，样本集合为 $$ S $$，分类类别集合为 $$ C=\{c_1, c_2, c_3, c_4\} $$

## 1. 预测结果的分类

对于所有分类模型（不仅仅是二分类），其中每一个分类类别的预测结果均可以分为四类： 

- **TP** —— True Positive
- **FP** —— Flase Positive
- **TN** —— True Negative
- **FN** —— False Negative

-|真实结果：$$c_1$$|真实结果：$$not \space c_1$$ 
:-:|:-:|:-:
**预测结果：$$c_1$$** | $$TP_{c_1}$$ | $$FP_{c_1}$$
**预测结果：$$not \space c_1$$** | $$FN_{c_1}$$ | $$TN_{c_1}$$

## 2. Accuracy

评估分类模型的预测结果，最直观的就是看预测的准确率，即 预测结果与真实结果相同的样本数量 在 总样本 中的占比，我们称之为 **Accuracy**【准确率】：

$$
Accuracy = \frac{\sum_Q TP}{|S|} = \frac{TP_{c_1} + TP_{c_2} + TP_{c_3} + TP_{c_4}}{|S|}
$$

一般来说，Accuracy 能直观的反映预测的情况。但是，我们很难量化的定义 Accuracy 的标准。也就是说，分类模型的 Accuracy 达到多少我们可以认为这个模型是优秀的？   
举个医疗领域的例子。假设某病症的发病率为 0.01%，如果分类模型对所有输入样本均返回“未发病”，那么 Accuracy 可达到 99.99%。但是，这么“高”的 Accuracy 没有任何意义。

上面的案例中，Accuracy 失效的原因之一，是由于其是针对所有分类类别统一分析准确率，这样会掩藏部分分类类别的“不准确”，尤其是样本数量少但非常重要的分类类别。由此，我们认识到需要对各个分类类别分开分析，这就是 Precision、Recall 和 F-measure。

## 3. Precision & Recall & F-measure

### 3.1. Precision

Accuracy 是所有分类类别的预测结果的准确率，而 Precision【精确率/查准率】是指某分类类别的预测结果的准确率，例如 $$c_1$$ 的 Precision 为：

$$
Precision_{c_1} = \frac{TP_{c_1}}{TP_{c_1}+FP_{c_1}} 
$$

一般来说，Precision 越高，分类模型越好。

但是，观察公式可以发现，如果把 $$c_1$$ 的 分类判别阈值 定的很高，只在非常有把握的情况下才将样本判别为 $$c_1$$，这样可以明显提高 $$ Precision_{c_1} $$。但是，这样我们会遗漏大量的真实分类为 $$c_1$$ 的样本。因此，引入了 Recall。

### 3.2. Recall

Recall【召回率/查全率】，顾名思义，是指真实结果为某分类类别的所有样本被准确找到的比例。例如 $$c_1$$ 的 Recall 为：

$$
Recall_{c_1} = \frac{TN_{c_1}}{TN_{c_1}+FP_{c_1}}
$$

一般来说，Recall 越高，分类模型越好。

但是，观察公式可以发现，如果把 $$c_1$$ 的 分类判别阈值 定的很低，甚至所有样本均判定为 $$c_1$$，这样 $$Recall_{c_1}$$ 可以达到很高。但实际上，这样并没有意义。

### 3.3. F-measure

通过 Precision 和 Recall 的分析，我们会发现两者一般情况下是此消彼长的关系：随着判别阈值的提高，Precision 逐渐提高，Recall 逐渐降低。因此，为了统一的评估分类模型，我们定义了 F-measure：

$$
\frac{2}{F\_measure} = \frac{1}{Precision} + \frac{1}{Recall}       
$$

$$
F\_measure = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}
$$

需要注意，F-measure 依然是针对某一分类类别而定义的。  
当然，如果是在二分类中，某一分类的 F-measure 基本可以确定另一分类的 F-measure。

对比 Accuracy 我们可以看出，F-measure 能更为明确的给出量化标准，例如 F-measure 达到 99.9% 就是好的模型，而 Accuracy 达到 99.9% 可能还没有意义。

## 4. PRC & AP & MAP

### 4.1. PRC(Precision Recall curve)

从上面的分析我们知道，随着判别阈值的变化，Precision 和 Recall 存在一定的此消彼长的关系。那么，它们之间的关系是怎样的呢？我们可以 Precision 为纵轴，Recall 为横轴，以每个样本的值（如概率）作为判别阈值，多个样本形成多个PR点，进而形成 PRC 曲线，以此来观察 Precision 和 Recall 的变化关系。

下图为两个分类模型的PRC曲线：
http://hi.csdn.net/attachment/201107/6/0_1309960406Ypy4.gif

得到 PRC 曲线后，我们怎么比较 分类模型 之间的优劣呢？   
一般来说，我们认为 PRC 曲线在上的更好，如上图中红色曲线更好。这是因为上方的曲线对同样的 Precision 有更高的 Recall，对同样的 Recall 有更高的 Precision。   
另外，相对光滑的曲线表现更为稳定。

### 4.2. AP(Average Precision)

观察 PRC 曲线较为抽象，既然认为上方的曲线更好，那么很容易想到以 PRC 曲线下的面积作为指标进行判定，这就是 AP(Average Precision)，这里的 Average 相当于是对 Precision 取均值。 

### 4.3. MAP(Mean Average Precision)

需要注意，Precision、Recall、PRC、AP 都是针对单个分类类别的，如果我们需要分析整体分类的情况，就需要对所有类别求均值，即
$$
MAP = \sum_{c in C} AP_c
$$

https://blog.csdn.net/pipisorry/article/details/51788927

http://alexkong.net/2013/06/introduction-to-auc-and-roc/

https://blog.csdn.net/jningwei/article/details/78955536

https://www.cnblogs.com/sddai/p/5696870.html

## 2. ROC(Receiver Operating Characteristic)

基于预测结果的四类，我们可以构建两个指标：

- $$TPR$$ : True Positive Rate，所有正样本中被预测为正的比例
    $$
    TPR = \frac{TP}{TP + FN}
    $$
- $$FPR$$ : False Positive Rate，所有负样本中被预测为正的比例
    $$
    FPR = \frac{FP}{TN + FP}
    $$

针对某一个类别，预测结果正确的比例
精确率


召回率


F-measure


准确率


<!-- toc -->

# 分类模型的评估

本文主要分析 分类模型 的评估标准。  
实际上，搜索模型也可以采用同样的评估标准，只需把每一次搜索作为一个分类即可。

> 本文以 $$ N $$ 分类举例，样本集合为 $$ S $$，分类类别集合为 $$ C=\{c_i, c_2, ... c_{n-1}, c_n\} $$

## 1. 预测结果的分类

对于所有分类模型（不仅仅是二分类），其中每一个分类类别的预测结果均可以分为四类：

- **TP** —— True Positive
- **FP** —— Flase Positive
- **TN** —— True Negative
- **FN** —— False Negative

-|真实结果：$$c_i$$|真实结果：$$not \space c_i$$
:-:|:-:|:-:
**预测结果：$$c_i$$** | $$TP_{c_i}$$ | $$FP_{c_i}$$
**预测结果：$$not \space c_i$$** | $$FN_{c_i}$$ | $$TN_{c_i}$$

## 2. 分类的判别阈值

分类模型，主要分别两个部分：
- **分类算法**

    分类算法 基于样本的业务指标计算得到一个分类指标（大多数算法是得到一个概率值）.

- **判别阈值**

    分类指标 基于 判别阈值 对样本进行分类.

也就是说，随着 判别阈值 的变化，样本分类也可能发生变化。同一分类算法，判别阈值 越高，$$TP,FP$$ 越少，$$FN,TN$$ 越多。

## 3. Accuracy

评估分类模型的预测结果，最直观的就是看预测的准确率，即 预测结果与真实结果相同的样本数量 在 总样本 中的占比，我们称之为 **Accuracy**【准确率】：

$$
Accuracy = \frac{\sum_{i = 1}^{n} TP_{c_i}}{|S|}
$$

一般来说，Accuracy 能直观的反映预测的情况。但是，我们很难量化的定义 Accuracy 的标准。也就是说，分类模型的 Accuracy 达到多少我们可以认为这个模型是优秀的？
举个医疗领域的例子。假设某病症的发病率为 0.01%，如果分类模型对所有输入样本均返回“未发病”（即发病的判别阈值>1），那么 Accuracy 可达到 99.99%。但是，这么“高”的 Accuracy 没有任何意义。

上面的案例中，Accuracy 失效的原因之一，是由于其是针对所有分类类别统一分析准确率，这样会掩藏部分分类类别的“不准确”，尤其是 **样本数量少但非常重要** 的分类类别。由此，我们认识到需要对各个分类类别分开分析，这就是 Precision、Recall 和 F-measure。

## 4. Precision & Recall & F-measure

### 4.1. Precision

Accuracy 是所有分类类别的预测结果的准确率，而 Precision【精确率/查准率】是指某分类类别的预测结果的准确率，例如 $$c_i$$ 的 Precision 为：

$$
Precision_{c_i} = \frac{TP_{c_i}}{TP_{c_i}+FP_{c_i}}
$$

一般来说，Precision 越高，此类别的分类效果越好。

但是，如果我们把 $$c_i$$ 的 分类判别阈值 定的很高，只在非常有把握的情况下才将样本判别为 $$c_i$$，这样可以明显提高 $$ Precision_{c_i} $$。但是，这样我们会遗漏大量的真实分类为 $$c_i$$ 的样本。因此，引入了 Recall。

### 4.2. Recall

Recall【召回率/查全率】，顾名思义，是指真实结果为某分类类别的所有样本被准确找到的比例。例如 $$c_i$$ 的 Recall 为：

$$
Recall_{c_i} = \frac{TP_{c_i}}{TP_{c_i}+FN_{c_i}}
$$

一般来说，Recall 越高，此类别的分类效果越好。

但是，如果我们把 $$c_i$$ 的 分类判别阈值 定的很低，甚至所有样本均判定为 $$c_i$$，这样 $$Recall_{c_i}$$ 可以达到很高。但实际上，这样并没有意义。

### 4.3. F-measure

理想情况下，我们希望 Precision 和 Recall 都很高。但实际场景中，两者一般是此消彼长的关系：随着判别阈值的提高，Precision 逐渐提高，Recall 逐渐降低。

因此，为了统一的评估分类模型，我们定义了 F-measure（也作F-score）：

$$
\frac{2}{F\_measure} = \frac{1}{Precision} + \frac{1}{Recall}
$$

$$
F\_measure = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall}
$$

需要注意，F-measure 依然是针对某一分类类别而定义的。

### 4.4. Summary

回到上面提到的病症。如果分类模型对所有输入样本均返回“未发病”（即发病的判别阈值>1），那么：

分类类别|Precision|Recall|F-measure
:-:|:-:|:-:|:-:
**发病** | $$0\%$$ | $$0\%$$ | -
**未发病** | $$99\%$$ | $$100\%$$ | $$99.5\%$$

对比 Accuracy 我们可以看出，Precision + Recall + F-measure 能细化到对每一个分类类别进行分析。


## 5. PRC & AP & MAP

### 5.1. PRC(Precision Recall Curve)

从上面的分析我们知道，随着判别阈值的变化，Precision 和 Recall 存在一定的此消彼长的关系。那么，它们之间的关系是怎样的呢？我们可以 Precision 为纵轴，Recall 为横轴，以每个样本的值（如概率）作为判别阈值，多个样本形成多个PR点，进而形成 PRC 曲线，以此来观察 Precision 和 Recall 的变化关系。下图为两个分类模型的PRC曲线：

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fxfiiqfqy6j309u07i748.jpg)

需要注意，PRC 曲线的自变量是 判别阈值，


接下来我们考虑PRC曲线的四个边界点：

1. **(0,0)**：Precision 为 0，Recall 为 0，说明此分类模型特别不好，
2. **(0,1)**：
3. **(1,0)**：
4. **(1,1)**：

得到 PRC 曲线后，我们怎么比较 分类模型 之间的优劣呢？
一般来说，我们认为 PRC 曲线在上的更好，如上图中红色曲线更好。这是因为上方的曲线对同样的 Precision 有更高的 Recall，对同样的 Recall 有更高的 Precision。
另外，相对光滑的曲线表现更为稳定。

### 5.2. AP(Average Precision)

观察 PRC 曲线较为抽象，既然认为上方的曲线更好，那么很容易想到以 PRC 曲线下的面积作为指标进行判定，这就是 AP(Average Precision)，这里的 Average 相当于是对 Precision 取均值。

### 5.3. MAP(Mean Average Precision)

需要注意，Precision、Recall、PRC、AP 都是针对单个分类类别的，如果我们需要分析整体分类的情况，就需要对所有类别求均值，即
$$
MAP_{c_i} = \frac{1}{|S|} \cdot \sum_{i=1}^{n} AP_{c_i}
$$

## 6. ROC & AUC

### 6.1. TPR & FPR

- $$TPR$$ : True Positive Rate，所有正样本中被预测为正的比例
，也就是 Recall
$$
TPR = \frac{TP}{TP + FN}
$$
- $$FPR$$ : False Positive Rate，所有负样本中被预测为正的比例
$$
FPR = \frac{FP}{TN + FP}
$$



### 6.2. ROC(Receiver Operating Characteristic)

ROC曲线的横坐标为 FPR，纵坐标为 TPR


### 6.3. AUC(Area Under ROC Curve)




https://blog.csdn.net/pipisorry/article/details/51788927

http://alexkong.net/2013/06/introduction-to-auc-and-roc/

https://blog.csdn.net/jningwei/article/details/78955536

https://www.cnblogs.com/sddai/p/5696870.html


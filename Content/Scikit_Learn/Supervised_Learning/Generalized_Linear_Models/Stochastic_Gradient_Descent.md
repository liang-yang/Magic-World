<!-- toc -->

# Stochastic Gradient Descent(SGD)

---

> see [梯度下降](https://zhuanlan.zhihu.com/p/36902908)

## 1. 最优化问题

最优化问题是求解函数极值的问题，包括极大值和极小值。

> 为统一分析，我们一般把求极大值转化为求极小值，如 $$ \max \big( f(x) \big) = \min \big( -f(x) \big).$$

一般来说，我们是通过微积分，对函数求导数为0的点。因为 导数为0 是 极值点 的必要条件（非充分条件）。

> 需要注意，一个函数可能有多个局部极值点，我们需要通过比较这些局部极值点才能找到全局极值点。

## 2. 梯度下降(Gradient Descent)

但是，当函数特别复杂时，求导为零很困难。例如：

$$
f(x,y) = x^3 - 2x^2 + e^{xy} - y^3 + 10y^2 + 100 \sin (xy)
$$

这个函数的求导为零就很难求解。由此，引入了 梯度下降 方法。

### 2.1. 梯度

**梯度** 是 多元函数 对各个自变量偏导数形成的向量，定义为：

$$
\qquad \nabla f(x_1,x_2,...,x_k) = \bigg( \frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_k} \bigg)^T
$$

其中 $$ \nabla $$ 称为梯度算子，它作用于一个多元函数，得到一个向量。 例如 $$ \nabla(x^2+xy-y^2) = (2x+y,x-2y) $$。

可导函数在某一点处取得极值的必要条件是梯度为零，梯度为零的点称为函数的驻点，这是疑似极值点。
需要注意，梯度为零只是函数取极值的必要条件而不是充分条件，即梯度为零的点可能不是极值点。如果需要确定是否极值点，还需要观察二阶导数：
- 如果二阶导数大于0，函数有极小值；
- 如果二阶导数小于0，函数有极大值；
- 如果二阶导数等于0，情况不定。

### 2.2. 梯度下降

对于复杂的函数，我们可以通过 迭代法，从一个初始点 $$ x_0 $$，反复使用某种规则从 $$ x_{k} $$ 移动到下一个点 $$ x_{k+1} $$，即

$$
\qquad x_{k+1} = h(x_{k})
$$

那么，具体的迭代规则是什么呢？

我们将多元函数 $$ f(x) $$ 在 $$ x_0 $$ 点处泰勒展开，有：

$$
\qquad f(x_0 + \Delta x) = f(x_0) + \big( \nabla f(x_0) \big)^T \Delta x + \omicron ( \Delta x )
$$

当 $$ \Delta x $$ 足够小时，我们可以忽略高次项 $$ \omicron ( \Delta x ) $$，可以得到：

$$
\qquad f(x_0 + \Delta x) - f(x_0) \approx \big( \nabla f(x) \big)^T \Delta x
$$

假设我们现在要求 极小值（求极大值可以转化为求极小值），则需要令函数递减：

$$
\qquad f(x_0 + \Delta x) - f(x_0) \approx \big( \nabla f(x) \big)^T \Delta x \leqslant 0
$$

即

$$
\qquad \big( \nabla f(x) \big)^T \Delta x = || \big( \nabla f(x) \big)^T || \cdot || \Delta x || \cdot \cos \theta \leqslant 0
$$

其中，$$ \theta $$ 是向量 $$ \big( \nabla f(x) \big)^T $$ 和 $$ \Delta x $$ 的夹角。 那么，只要 $$ \cos \theta \leqslant 0 $$ 即可。

特别的，在 $$ \theta = \pi $$，即向量 $$ \Delta x $$ 与梯度 $$ \big( \nabla f(x) \big)^T $$ 反向时，$$ \cos \theta = -1 $$，就可以更快的朝极小值移动。因此，我们令

$$
\Delta x = - \eta \cdot \big( \nabla f(x) \big)^T
$$

则必有 $$ \cos \theta = -1 $$。其中，$$ \big( \nabla f(x) \big)^T $$ 就是梯度，$$ \eta $$ 我们称之为 步长，以此控制 $$ \Delta x $$ 足够小。

### 2.3. 迭代的意义

梯度下降，表面上是迭代变量，每一次不断的修改变量。

实际上，**迭代的意义是迭代梯度，不断的修正梯度**。

### 2.4. 结束点

梯度下降 的方法就是不断的迭代，那么，迭代到什么时候停止呢？

理想情况下，迭代到 梯度 为零的点即为极值点。

实际应用时，我们有两种方案：

1. 可以指定一个阈值，更新后的梯度在阈值范围内即停止；
2. 指定迭代次数，一旦达到迭代次数，无论是否找到极值均结束；

## 3. 两个问题

### 3.1. 局部极值

如果 函数 存在多个极值点，梯度下降方法可能找到的是局部极值点，而不是全局极值点。

一般情况下，我们可以选取多个随机点作为起始点，多计算几次，最终比较出哪个是全局极值点。

### 3.2. 鞍点(Saddle Point)问题

鞍点的数学含义是：目标函数在此点上的梯度（一阶导数）值为零， 但从该点出发的一个方向是函数的极大值点，而在另一个方向是函数的极小值点。

我们考虑如下函数：

$$
\qquad f(x,y) = x^2 - y^2
$$

显然点 $$(0,0)$$ 既不是极大值又不是极小值，但是其梯度在此点为零，算法会结束。这个点就称为 鞍点。

## 4. 批量梯度下降(Batch Gradient Descent)

机器学习中，一般是求样本集的损失函数的极小值，我们就通过 梯度下降方法 来求解。

假设样本集为 $$ S = \{x_1,x_2,...,x_N\} $$，单个样本的损失函数为 $$ L(w,x_i) $$，那么总的损失函数为：

$$
\qquad L(w) = \sum_{i=1}^N L(w,x_i)
$$

其中，$$ w $$ 为我们需要求解的参数向量。

梯度下降 算法步骤：

1. 对参数向量 $$w$$ 取一个随机初始值 $$w_0$$；
2. 对 $$ L(w) $$ 参数向量 $$ w $$ 的每个分量求微分，代入 $$S$$ 和 $$w_0$$，求得梯度 $$ d_0 $$；
3. 更新参数向量 $$w$$：$$w_1 = w_0 - \eta \cdot w_0$$；
4. 重复步骤2、步骤3，一直到梯度为零；

以上的方法，我们称为 批量梯度下降。“批量”的意思，是指每一步迭代都会代入样本集 $$S$$ 中的所有样本。

很明显，当样本集很大时，每一轮迭代都全量计算，消耗的 计算资源 和 时间 都比较大。因此，我们引入了 随机梯度下降。

## 5. 随机梯度下降(Stochastic Gradient Descent)

批量梯度下降（BGD） 的问题是每一轮迭代都需要计算全量样本集，目的是为了获取全局梯度，使得每一轮迭代都能比较准确的移动。

但是，并不是每一轮迭代都需要考虑所有样本点，尤其是在微调阶段，绝大部分样本点都是无用的。因此，我们极端的处理：每一轮迭代仅计算单个样本点，单个样本点达到梯度为零后再计算下一样本点，不断迭代到所有样本点，这就是 **随机梯度下降（SGD）**。

- **收敛性**

通过之前的分析，我们可以理解 BGD 是肯定收敛到极小值的，那么 SGD 是否收敛呢？答案是肯定的，具体可参考 [为什么随机梯度下降方法能够收敛？](https://www.zhihu.com/question/27012077)

- **优缺点**

相对于 BGD，SGD 可能会增加迭代的轮数，但每一轮迭代的计算量大大的降低了，总体来看效率高很多，这是最大的优点。
但是，由于每一轮迭代仅考虑单个样本点，而此样本点计算的梯度不一定是朝着极小化的方向移动，因此可能使得整个迭代过程出现反复，即噪音较多。但总体移动方向肯定是朝着极小化的方向移动的。

为解决 SGD 的噪音问题，引入了 小批量梯度下降法。

## 6. 小批量梯度下降法(Mini-Batch Gradient Descent)

BGD 取全量样本集会导致训练慢，SGD 取单个样本点会导致噪音大，因此两者结合，每轮迭代从样本集中随机取部分样本点形成一个小数据量的训练集，就可以同时规避以上两个的问题，这就是 小批量梯度下降法（MBGD）。